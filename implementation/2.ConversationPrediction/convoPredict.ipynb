{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your locally saved fine-tuned GPT-2 model with a classification head\n",
    "MODEL_PATH = \"gpt2-conversation-finetuned\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)  # GPT2ForSequenceClassification is for classification, not generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(prompt_text: str, model, tokenizer, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates the next part of the conversation given a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt_text (str): The initial conversation/prompt.\n",
    "        model: A fine-tuned GPT-2 model loaded for text generation.\n",
    "        tokenizer: The tokenizer associated with the GPT-2 model.\n",
    "        max_length (int): The maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The continuation text generated by GPT-2.\n",
    "    \"\"\"\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "    # Generate continuation\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,             # Use beam search for higher quality generation\n",
    "            no_repeat_ngram_size=2,  # Helps reduce repetition\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the output tokens to text\n",
    "    continuation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ashansubodha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashansubodha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashansubodha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_conversation_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input conversation text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation and digits\n",
    "    3. Tokenizing\n",
    "    4. Removing stopwords\n",
    "    5. Removing short tokens (optional)\n",
    "    \n",
    "    Args:\n",
    "        text (str): The conversation text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation and digits\n",
    "    # This regex replaces any character that is not a letter or whitespace\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 5. (Optional) Remove short tokens of length 1\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    # Join back into a single string\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Original Prompt -----\n",
      "Hello, I have a question about my bank account. Could you help me figure out why my card was declined?\n",
      "\n",
      "----- Model Continuation -----\n",
      "hello question bank account could help figure card declined. Can you provide more details about your account?\n",
      "Yes, we can provide a contact number for you.\n",
      "Thank you for taking the time to provide us with this information. We appreciate your interest in helping us. Please contact us for more information about this matter.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example prompt (conversation start)\n",
    "    prompt = (\n",
    "        \"Hello, I have a question about my bank account. \"\n",
    "        \"Could you help me figure out why my card was declined?\"\n",
    "    )\n",
    "    \n",
    "    cleaned_prompt = clean_conversation_text(prompt)\n",
    "\n",
    "    # Generate the continuation\n",
    "    max_length = 100  # Adjust max_length as needed\n",
    "    generated_text = continue_conversation(cleaned_prompt, model, tokenizer, max_length)\n",
    "\n",
    "    print(\"----- Original Prompt -----\")\n",
    "    print(prompt)\n",
    "    print(\"\\n----- Model Continuation -----\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voipvishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
