{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # T5 Conversation Completion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1) Install and Import Dependencies\n",
    "# =========================================\n",
    "# If you're in a fresh environment (e.g. Google Colab), you may need:\n",
    "# !pip install transformers datasets accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CONVERSATION_ID  CONVERSATION_STEP  \\\n",
      "0                6                  1   \n",
      "1                6                  2   \n",
      "2                6                  3   \n",
      "3                6                  4   \n",
      "4                6                  5   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  Good morning, this is [Your Name]'s personal a...   \n",
      "1  Hello, my name is Jamie. I'm interested in vol...   \n",
      "2  Yes, I'm really passionate about environmental...   \n",
      "3  Great, how do I sign up, and where can I find ...   \n",
      "4  Could you send me the link, please? And my ema...   \n",
      "\n",
      "                             CONTEXT    LABEL                      FEATURES  \\\n",
      "0          Standard opening exchange  neutral                           NaN   \n",
      "1   Encourages the caller's interest  neutral      welcoming, positive_tone   \n",
      "2    Reinforces anyone can volunteer  neutral                     inclusive   \n",
      "3           Demonstrates flexibility  neutral  helpful_tone, offers_options   \n",
      "4  Fulfills caller's request quickly  neutral                 prompt_action   \n",
      "\n",
      "  ANNOTATIONS  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 2) Load the Conversation CSV\n",
    "# =========================================\n",
    "# We'll assume your CSV has:\n",
    "# CONVERSATION_ID, CONVERSATION_STEP, TEXT, ...\n",
    "# We'll build pairs of (partial_conversation -> full_conversation) for training.\n",
    "\n",
    "df = pd.read_csv(\"/Users/ashansubodha/Desktop/VOIP Vishing/conversation-prediction/BETTER30.csv\")\n",
    "\n",
    "# Let's see what columns we have\n",
    "print(df.head())\n",
    "\n",
    "# CONVERSATION_ID | CONVERSATION_STEP | TEXT | CONTEXT | LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3) Build (source, target) pairs\n",
    "# =========================================\n",
    "# We want:\n",
    "# source = partial conversation (e.g., first 30% or 50%)\n",
    "# target = entire conversation\n",
    "#\n",
    "# For demonstration:\n",
    "#  - We'll group by CONVERSATION_ID\n",
    "#  - We'll take the first X% of TEXT lines as \"partial\"\n",
    "#  - We'll take the entire conversation as \"target\"\n",
    "# This is just one approach. You could also take partial => remaining turns.\n",
    "\n",
    "def build_partial_full_pairs(df, partial_ratio=0.3):\n",
    "    \"\"\"\n",
    "    For each conversation:\n",
    "      1. Sort by CONVERSATION_STEP\n",
    "      2. partial_convo = first partial_ratio % of lines\n",
    "      3. full_convo = all lines\n",
    "      4. Return (partial_convo_text, full_convo_text)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    grouped = df.groupby(\"CONVERSATION_ID\")\n",
    "\n",
    "    for convo_id, group in grouped:\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "\n",
    "        # Convert entire conversation to a single string\n",
    "        full_convo = \"\\n\".join(texts)\n",
    "\n",
    "        # Determine partial slice\n",
    "        cutoff = max(1, int(len(texts) * partial_ratio))  # at least 1 line\n",
    "        partial_texts = texts[:cutoff]\n",
    "        partial_convo = \"\\n\".join(partial_texts)\n",
    "\n",
    "        rows.append({\n",
    "            \"source\": partial_convo,\n",
    "            \"target\": full_convo\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pairs_df = build_partial_full_pairs(df, partial_ratio=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 65 conversation pairs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Built {len(pairs_df)} conversation pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, this is [Your Name]'s personal assistan...</td>\n",
       "      <td>Hello, this is [Your Name]'s personal assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello, this is the assistant to [Your Name]. H...</td>\n",
       "      <td>Hello, this is the assistant to [Your Name]. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, this is the office of [Your Name]. How ...</td>\n",
       "      <td>Hello, this is the office of [Your Name]. How ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello, you've reached the assistant for [Your ...</td>\n",
       "      <td>Hello, you've reached the assistant for [Your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hello, you've reached the assistant to [Your N...</td>\n",
       "      <td>Hello, you've reached the assistant to [Your N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Good afternoon, you've reached [Your Name]'s a...</td>\n",
       "      <td>Good afternoon, you've reached [Your Name]'s a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Hello, you've reached the office of [Your Name...</td>\n",
       "      <td>Hello, you've reached the office of [Your Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Good morning, [Your Name]'s office. How may I ...</td>\n",
       "      <td>Good morning, [Your Name]'s office. How may I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Hello, you've reached [Your Name]'s assistant....</td>\n",
       "      <td>Hello, you've reached [Your Name]'s assistant....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "0   Hello, this is [Your Name]'s personal assistan...   \n",
       "1   Hello, this is the personal assistant of [Your...   \n",
       "2   Hello, this is the assistant to [Your Name]. H...   \n",
       "3   Hello, this is the office of [Your Name]. How ...   \n",
       "4   Hello, you've reached the assistant for [Your ...   \n",
       "..                                                ...   \n",
       "60  Hello, you've reached the assistant to [Your N...   \n",
       "61  Good afternoon, you've reached [Your Name]'s a...   \n",
       "62  Hello, you've reached the office of [Your Name...   \n",
       "63  Good morning, [Your Name]'s office. How may I ...   \n",
       "64  Hello, you've reached [Your Name]'s assistant....   \n",
       "\n",
       "                                               target  \n",
       "0   Hello, this is [Your Name]'s personal assistan...  \n",
       "1   Hello, this is the personal assistant of [Your...  \n",
       "2   Hello, this is the assistant to [Your Name]. H...  \n",
       "3   Hello, this is the office of [Your Name]. How ...  \n",
       "4   Hello, you've reached the assistant for [Your ...  \n",
       "..                                                ...  \n",
       "60  Hello, you've reached the assistant to [Your N...  \n",
       "61  Good afternoon, you've reached [Your Name]'s a...  \n",
       "62  Hello, you've reached the office of [Your Name...  \n",
       "63  Good morning, [Your Name]'s office. How may I ...  \n",
       "64  Hello, you've reached [Your Name]'s assistant....  \n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pairs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 52\n",
      "Eval examples: 13\n",
      "Sample train example:\n",
      " {'source': \"Hello, this is the assistant to [Your Name]. How may I assist you today?\\nGood day! I'm Sarah from the Global Aid Foundation. We're currently running a campaign to support victims of the recent natural disaster in [Region]. We're reaching out to compassionate individuals like [Your Name] for support.\", 'target': \"Hello, this is the assistant to [Your Name]. How may I assist you today?\\nGood day! I'm Sarah from the Global Aid Foundation. We're currently running a campaign to support victims of the recent natural disaster in [Region]. We're reaching out to compassionate individuals like [Your Name] for support.\\nAbsolutely. Our foundation has been actively involved in various relief efforts worldwide. This particular campaign focuses on providing shelter, food, and medical supplies to the affected individuals in [Region].\\nWe're in the process of updating our website with the latest campaign details. However, we can assure you that 100% of the donations go directly to the cause.\\nI understand your need for verification, but time is of the essence, and we're trying to mobilize resources quickly. A direct donation over the phone can make an immediate impact.\\nWhile we appreciate your thoroughness, we were hoping for immediate support given the urgent situation. We can provide you with a donation link right now.\\nI'll see what I can do about sending you the information. However, please consider acting quickly as every moment counts for those in need.\\nThank you for your understanding, and we hope to have your support soon. Goodbye.\"}\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 4) Create a Hugging Face Dataset\n",
    "# =========================================\n",
    "dataset = Dataset.from_pandas(pairs_df[[\"source\", \"target\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset  = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train examples:\", len(train_dataset))\n",
    "print(\"Eval examples:\",  len(eval_dataset))\n",
    "print(\"Sample train example:\\n\", train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5) Prepare the T5 Tokenizer\n",
    "# =========================================\n",
    "model_name = \"t5-small\"  # or \"t5-base\", \"flan-t5-base\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# T5 uses an <extra_id_0> style for mask, etc., but for general generation,\n",
    "# we just need the normal tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/52 [00:00<?, ? examples/s]/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:00<00:00, 941.14 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1172.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 8774,     6,    48,    19,     8,  6165,    12,   784, 21425,  5570,\n",
      "         4275,   571,   164,    27,  2094,    25,   469,    58,  1804,   239,\n",
      "           55,    27,    31,    51,  8077,    45,     8,  3699, 12090,  2941,\n",
      "            5,   101,    31,    60,  1083,  1180,     3,     9,  2066,    12,\n",
      "          380,  8926,    13,     8,  1100,   793,  6912,    16,   784, 17748,\n",
      "           23,   106,  4275,   101,    31,    60,  7232,    91,    12, 21801,\n",
      "         1742,   114,   784, 21425,  5570,   908,    21,   380,     5,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([ 8774,     6,    48,    19,     8,  6165,    12,   784, 21425,  5570,\n",
      "         4275,   571,   164,    27,  2094,    25,   469,    58,  1804,   239,\n",
      "           55,    27,    31,    51,  8077,    45,     8,  3699, 12090,  2941,\n",
      "            5,   101,    31,    60,  1083,  1180,     3,     9,  2066,    12,\n",
      "          380,  8926,    13,     8,  1100,   793,  6912,    16,   784, 17748,\n",
      "           23,   106,  4275,   101,    31,    60,  7232,    91,    12, 21801,\n",
      "         1742,   114,   784, 21425,  5570,   908,    21,   380,     5, 20510,\n",
      "            5,   421,  3361,    65,   118,  8609,  1381,    16,   796,  4956,\n",
      "         2231,  4388,     5,   100,  1090,  2066,     3,  6915,    30,  1260,\n",
      "         8596,     6,   542,     6,    11,  1035,  4471,    12,     8,  4161,\n",
      "         1742,    16,   784, 17748,    23,   106,  4275,   101,    31,    60,\n",
      "           16,     8,   433,    13, 14809,    69,   475,    28,     8,  1251,\n",
      "         2066,  1030,     5,   611,     6,    62,    54,  7992,    25,    24,\n",
      "         2349,    13,     8, 10712,   281,  1461,    12,     8,  1137,     5,\n",
      "           27,   734,    39,   174,    21, 17549,     6,    68,    97,    19,\n",
      "           13,     8, 10848,     6,    11,    62,    31,    60,  1119,    12,\n",
      "        26169,    15,  1438,  1224,     5,    71,  1223,  9294,   147,     8,\n",
      "          951,    54,   143,    46,  5299,  1113,     5,   818,    62,  3653,\n",
      "           39,  9517,   655,     6,    62,   130,  6055,    21,  5299,   380,\n",
      "          787,     8, 10839,  1419,     5,   101,    54,   370,    25,    28,\n",
      "            3,     9,  9294,  1309,   269,   230,     5,    27,    31,   195,\n",
      "          217,   125,    27,    54,   103,    81,  5657,    25,     8,   251,\n",
      "            5,   611,     6,   754,  1099,  6922,  1224,    38,   334,   798,\n",
      "        12052,    21,   273,    16,   174,     5,  1562,    25,    21,    39,\n",
      "         1705,     6,    11,    62,   897,    12,    43,    39,   380,  1116,\n",
      "            5,  1804,   969,    15,     5,     1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6) Tokenization Function\n",
    "# =========================================\n",
    "# We'll treat \"source\" as the encoder input, \"target\" as the decoder output.\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # examples[\"source\"] -> list of partial conversation strings\n",
    "    # examples[\"target\"] -> list of full conversation strings\n",
    "\n",
    "    # Encode the source\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=512,      # adjust as needed\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Encode the target\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            max_length=512,   # adjust if conversations can be long\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset  = eval_dataset.map(preprocess_function,  batched=True)\n",
    "\n",
    "# Remove original columns to keep only tokenized fields\n",
    "train_dataset = train_dataset.remove_columns([\"source\",\"target\"])\n",
    "eval_dataset  = eval_dataset.remove_columns([\"source\",\"target\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7) Data Collator for Seq2Seq\n",
    "# =========================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_name,\n",
    "    padding=\"longest\",  # or \"max_length\"\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 8) Load T5 Model\n",
    "# =========================================\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=t5-conversation-model/runs/Jan03_16-23-31_Ashans-MacBook-Pro.local,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=100,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=t5-conversation-model,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=2,\n",
       "per_device_train_batch_size=2,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=t5-conversation-model,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.EPOCH,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# 9) Training Arguments\n",
    "# =========================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5-conversation-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 10) Define Trainer\n",
    "# =========================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78 [00:00<?, ?it/s]/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 26/78 [00:37<00:43,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0752596855163574, 'eval_runtime': 4.3614, 'eval_samples_per_second': 2.981, 'eval_steps_per_second': 1.605, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 52/78 [00:55<00:14,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8930037021636963, 'eval_runtime': 0.545, 'eval_samples_per_second': 23.853, 'eval_steps_per_second': 12.844, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [01:11<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8547608852386475, 'eval_runtime': 0.5639, 'eval_samples_per_second': 23.056, 'eval_steps_per_second': 12.415, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [01:12<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 72.6488, 'train_samples_per_second': 2.147, 'train_steps_per_second': 1.074, 'train_loss': 3.6464949387770433, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=78, training_loss=3.6464949387770433, metrics={'train_runtime': 72.6488, 'train_samples_per_second': 2.147, 'train_steps_per_second': 1.074, 'total_flos': 3755734990848.0, 'train_loss': 3.6464949387770433, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# 11) Train the Model\n",
    "# =========================================\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-conversation-finetuned/tokenizer_config.json',\n",
       " 't5-conversation-finetuned/special_tokens_map.json',\n",
       " 't5-conversation-finetuned/spiece.model',\n",
       " 't5-conversation-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "trainer.save_model(\"t5-conversation-finetuned\")\n",
    "tokenizer.save_pretrained(\"t5-conversation-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Conversation ===\n",
      "Caller: Good Morning, I'm Sanuja from State Bank of Sri Lanka. Callee: Oh, hi. I'm in meeting now. Could you call later? Caller: Good Morning, I'm Sanuja from State Bank of Sri Lanka. Callee: Oh, hi. I'm in meeting now. Could you call later? Caller: Good Morning, I'm Sanuja from State Bank of Sri Lanka. Callee: Oh, hi. I'm in meeting now. Could\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 12) Inference / Generation\n",
    "# =========================================\n",
    "# Suppose we have a partial conversation snippet, and want T5 to generate\n",
    "# the entire conversation (or next lines).\n",
    "\n",
    "partial_text = \"\"\"\n",
    "Caller: Good Morning, I'm Sanuja from State Bank of Sri Lanka.\n",
    "Callee: Oh, hi. I'm in a meeting now. Could you call later?\n",
    "Caller:\n",
    "\"\"\"\n",
    "# This is our \"source.\" The model should generate the \"target\" (the full convo).\n",
    "\n",
    "# Load model if needed\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-conversation-finetuned\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-conversation-finetuned\")\n",
    "# model.eval()\n",
    "\n",
    "encoded_input = tokenizer.encode(\n",
    "    partial_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "encoded_input = encoded_input.cuda() if torch.cuda.is_available() else encoded_input\n",
    "\n",
    "outputs = model.generate(\n",
    "    encoded_input,\n",
    "    max_length=200,       # set a max length for generation\n",
    "    num_beams=4,          # or do_sample=True for sampling\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Conversation ===\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 12) Inference / Generation\n",
    "# =========================================\n",
    "# Suppose we have a partial conversation snippet, and want T5 to generate\n",
    "# the entire conversation (or next lines).\n",
    "\n",
    "partial_text = \"\"\"\n",
    "Caller: Good Morning, I'm Sanuja from State Bank of Sri Lanka.\n",
    "Callee: Oh, hi. I'm in a meeting now. Could you call later?\n",
    "Caller:\n",
    "\"\"\"\n",
    "# This is our \"source.\" The model should generate the \"target\" (the full convo).\n",
    "\n",
    "# Load model if needed\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-conversation-finetuned\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-conversation-finetuned\")\n",
    "# model.eval()\n",
    "\n",
    "# Check if MPS is available\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "encoded_input = tokenizer.encode(\n",
    "    partial_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(device)  # move tensor to the same device\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        encoded_input,\n",
    "        max_length=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_input = encoded_input.cuda() if torch.cuda.is_available() else encoded_input\n",
    "\n",
    "outputs = model.generate(\n",
    "    encoded_input,\n",
    "    max_length=200,       # set a max length for generation\n",
    "    num_beams=4,          # or do_sample=True for sampling\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Conversation ===\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voipvishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
