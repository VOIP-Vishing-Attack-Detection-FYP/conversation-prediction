{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFine-Tuning Llama 2 for Conversation Prediction (Partial→Remainder)\\n\\nSteps:\\n1) Read CSV conversation data.\\n2) Build partial->remainder strings.\\n3) Concatenate partial + remainder into a single text sample so Llama can learn\\n   to predict remainder given partial in a causal LM fashion.\\n4) Use the 'LlamaTokenizer' and 'LlamaForCausalLM' from Hugging Face.\\n5) Fine-tune with Trainer (or custom loop).\\n6) Provide comments on parameter changes for easy tuning.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-Tuning Llama 2 for Conversation Prediction (Partial→Remainder)\n",
    "\n",
    "Steps:\n",
    "1) Read CSV conversation data.\n",
    "2) Build partial->remainder strings.\n",
    "3) Concatenate partial + remainder into a single text sample so Llama can learn\n",
    "   to predict remainder given partial in a causal LM fashion.\n",
    "4) Use the 'LlamaTokenizer' and 'LlamaForCausalLM' from Hugging Face.\n",
    "5) Fine-tune with Trainer (or custom loop).\n",
    "6) Provide comments on parameter changes for easy tuning.\n",
    "\"\"\"\n",
    "\n",
    "# If needed, install requirements in a new environment:\n",
    "# !pip install transformers accelerate bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1) Imports\n",
    "# ======================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1) Imports\n",
    "# ======================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 3) Prepare a Single-Text Format\n",
    "# ======================================\n",
    "# We'll define a small function that merges partial + remainder\n",
    "# into a single text. The model will treat the partial as context/prompt,\n",
    "# then learn to predict the remainder tokens.\n",
    "\n",
    "def make_single_text(partial, remainder, sep=\"\\n\"):\n",
    "    \"\"\"\n",
    "    We'll just put partial + newline + remainder for training.\n",
    "    During inference, you'd prompt with partial and let the model generate the remainder.\n",
    "    \"\"\"\n",
    "    return partial + sep + remainder\n",
    "\n",
    "class ConversationPredictionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a single 'text' that includes partial + remainder.\n",
    "    We'll let Llama do causal LM training.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        for (part, rem) in pairs:\n",
    "            if len(part.strip())==0 or len(rem.strip())==0:\n",
    "                continue\n",
    "            full_text = make_single_text(part, rem)  \n",
    "            self.examples.append(full_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# We'll define a data collator that does the tokenizing/truncation in __call__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 4) Data Collator with LM\n",
    "# ======================================\n",
    "class ConversationCollator:\n",
    "    \"\"\"\n",
    "    We'll let the collator do the tokenization on the fly\n",
    "    so we only hold strings in memory, not big token lists.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch_texts):\n",
    "        # batch_texts is a list of strings (partial+remainder)\n",
    "        encoding = self.tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        return encoding\n",
    "\n",
    "# Then we can rely on DataCollatorForLanguageModeling or just do a standard approach:\n",
    "# But for a causal LM approach, we do NOT do masked LM; we do full next-token prediction.\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# We'll define a \"DataCollatorForLanguageModeling\" with mlm=False, so it sets up\n",
    "# the appropriate labels for causal LM training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, we can rely on a simpler approach: \n",
    "# The GPT-like approach is: input_ids = output_ids for causal LM. \n",
    "# If you want to specifically ignore the partial portion from the loss,\n",
    "# you'd do a custom approach. But let's do standard approach that \n",
    "# trains on the entire sequence. The partial portion is also predicted,\n",
    "# but the model can handle that. \n",
    "#\n",
    "# We'll keep it straightforward: \n",
    "#   text -> encode -> (input_ids, labels are the same) \n",
    "# Let the standard \"DataCollatorForLanguageModeling(mlm=False)\" handle it.\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def build_dataset_and_collator(pairs, tokenizer, max_length=512):\n",
    "    dataset = ConversationPredictionDataset(pairs, tokenizer, max_length)\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    return dataset, collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or \"meta-llama/Llama-2-7b-chat-hf\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# We load the Llama tokenizer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Llama might not define a pad token by default, so let's set it:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 5) Loading Llama2, Tokenizer, Building Datasets\n",
    "# ======================================\n",
    "# We'll pick a Llama 2 checkpoint on Hugging Face. For example:\n",
    "# \"meta-llama/Llama-2-7b-hf\" or \"meta-llama/Llama-2-7b-chat-hf\" if you have rights to it.\n",
    "# Make sure you have accepted the license on Hugging Face and have an access token if needed.\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# We load the Llama tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "# Llama might not define a pad token by default, so let's set it:\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"  # typical for causal LM\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# Build dataset + collator\n",
    "max_length = 512  # <--- you can adjust this depending on GPU memory\n",
    "ds, data_collator = build_dataset_and_collator(pairs, tokenizer, max_length)\n",
    "\n",
    "# train_test_split if you want\n",
    "train_size = int(0.9*len(ds))\n",
    "eval_size = len(ds)-train_size\n",
    "train_ds, eval_ds = torch.utils.data.random_split(ds, [train_size, eval_size])\n",
    "print(f\"Train size = {len(train_ds)}, Eval size = {len(eval_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 6) Llama2 Model for Causal LM\n",
    "# ======================================\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",   # <--- if using accelerate, bitsandbytes, etc.\n",
    "    torch_dtype=torch.float16  # or \"auto\"\n",
    ")\n",
    "# We assume you have the GPU VRAM for it. Otherwise, consider LoRA or 4-bit etc.\n",
    "\n",
    "# We'll define the training arguments\n",
    "\n",
    "# Comments on param lines so you can tune them easily:\n",
    "num_epochs = 1  # <--- Increase for better convergence\n",
    "batch_size = 1  # <--- Adjust batch size for your GPU memory\n",
    "lr = 1e-4       # <--- Tune your learning rate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
