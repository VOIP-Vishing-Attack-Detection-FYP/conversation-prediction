{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvaluate a fine-tuned BART model (partial->entire conversation) using BLEU & ROUGE.\\n\\nSteps:\\n1) Build partial->entire from CSV\\n2) Load fine-tuned BART\\n3) Generate predicted entire conversation from partial\\n4) Compare with reference entire conversation using BLEU & ROUGE\\n5) (Optional) visualize BLEU distribution\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate a fine-tuned BART model (partial->entire conversation) using BLEU & ROUGE.\n",
    "\n",
    "Steps:\n",
    "1) Build partial->entire from CSV\n",
    "2) Load fine-tuned BART\n",
    "3) Generate predicted entire conversation from partial\n",
    "4) Compare with reference entire conversation using BLEU & ROUGE\n",
    "5) (Optional) visualize BLEU distribution\n",
    "\"\"\"\n",
    "\n",
    "# Required installations (if needed):\n",
    "# !pip install nltk evaluate sentencepiece\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1) Imports\n",
    "# =========================================\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Path: C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\n",
      "Model Path: C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/fineTune/bart_partial_entire_model\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 2) Paths\n",
    "# =========================================\n",
    "csv_path = r\"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"\n",
    "model_path = r\"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/fineTune/bart_partial_entire_model\"\n",
    "\n",
    "partial_ratio = 0.5  # <--- ratio for partial snippet\n",
    "print(\"CSV Path:\", csv_path)\n",
    "print(\"Model Path:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->entire pairs: 76\n",
      "\n",
      "Sample pair:\n",
      "Partial: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? \n",
      "Entire: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process?\n",
      "You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee?\n",
      "Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com.\n",
      "Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help!\n",
      "You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3) Build partial->entire from CSV\n",
    "# =========================================\n",
    "def build_partial_entire(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    For each conversation:\n",
    "      partial snippet = first partial_ratio lines\n",
    "      entire snippet  = all lines\n",
    "    Returns list of (partial_str, entire_str)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        lines = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(lines) < 2:\n",
    "            continue\n",
    "\n",
    "        # entire conversation\n",
    "        entire_str = \"\\n\".join(lines).strip()\n",
    "\n",
    "        # partial snippet\n",
    "        cutoff = max(1, int(len(lines)*partial_ratio))\n",
    "        partial_lines = lines[:cutoff]\n",
    "        partial_str = \"\\n\".join(partial_lines).strip()\n",
    "\n",
    "        if partial_str and entire_str:\n",
    "            pairs.append((partial_str, entire_str))\n",
    "    return pairs\n",
    "\n",
    "pairs = build_partial_entire(csv_path, partial_ratio)\n",
    "print(\"Number of partial->entire pairs:\", len(pairs))\n",
    "if pairs:\n",
    "    print(\"\\nSample pair:\\nPartial:\", pairs[0][0], \"\\nEntire:\", pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model from: C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/fineTune/bart_partial_entire_model\n",
      "BART model loaded.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 4) Load Fine-Tuned BART\n",
    "# =========================================\n",
    "print(\"Loading tokenizer and model from:\", model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"BART model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 5) Generate Entire Conversation from Partial\n",
    "# =========================================\n",
    "def generate_entire_conversation(model, tokenizer, partial_text, max_new_tokens=100, num_beams=4):\n",
    "    \"\"\"\n",
    "    Encodes the partial snippet, uses model.generate to produce entire conversation\n",
    "    (the model was trained so partial->entire).\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        partial_text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return gen_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating & evaluating for each partial->entire pair...\n",
      "\n",
      "=== Results ===\n",
      "Average BLEU on test set = 0.1778\n",
      "ROUGE-1 F1 = 0.5057\n",
      "ROUGE-2 F1 = 0.4930\n",
      "ROUGE-L F1 = 0.5040\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6) Evaluate with BLEU & ROUGE\n",
    "# =========================================\n",
    "# We'll do sample-level BLEU and store them in a list\n",
    "# Then we'll do a corpus-level ROUGE with \"evaluate\" library\n",
    "\n",
    "rouge_evaluator = evaluate.load(\"rouge\")\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "\n",
    "bleu_scores = []\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"Generating & evaluating for each partial->entire pair...\")\n",
    "\n",
    "for i, (partial_str, entire_ref) in enumerate(pairs):\n",
    "    # 1) Generate entire conversation from partial\n",
    "    predicted_entire = generate_entire_conversation(model, tokenizer, partial_str)\n",
    "    \n",
    "    # store for ROUGE\n",
    "    predictions.append(predicted_entire)\n",
    "    references.append(entire_ref)\n",
    "\n",
    "    # compute sample-level BLEU\n",
    "    ref_tokens = nltk.word_tokenize(entire_ref.lower())\n",
    "    hyp_tokens = nltk.word_tokenize(predicted_entire.lower())\n",
    "    if len(ref_tokens)==0 or len(hyp_tokens)==0:\n",
    "        bleu = 0.0\n",
    "    else:\n",
    "        bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothing)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# now corpus-level ROUGE\n",
    "rouge_results = rouge_evaluator.compute(predictions=predictions, references=references)\n",
    "# typical keys: 'rouge1', 'rouge2', 'rougeL', 'rougeLsum'\n",
    "rouge1 = rouge_results[\"rouge1\"]\n",
    "rouge2 = rouge_results[\"rouge2\"]\n",
    "rougeL = rouge_results[\"rougeL\"]\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "print(f\"\\n=== Results ===\")\n",
    "print(f\"Average BLEU on test set = {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1 F1 = {rouge1:.4f}\")\n",
    "print(f\"ROUGE-2 F1 = {rouge2:.4f}\")\n",
    "print(f\"ROUGE-L F1 = {rougeL:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7) Visualize BLEU distribution\n",
    "# =========================================\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(bleu_scores, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Sample-Level BLEU Scores\")\n",
    "plt.xlabel(\"BLEU Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
