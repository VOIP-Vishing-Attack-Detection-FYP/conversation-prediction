{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReusing a Saved Encoder-Decoder Model and Tokenizer in Another Notebook\\n\\nSteps:\\n1) Load your saved tokenizer.\\n2) Construct the same architecture for the encoder-decoder.\\n3) Load the model state_dict from disk.\\n4) Write a simple function to generate the remainder (partial -> remainder).\\n5) Test on a partial snippet to confirm the model works.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reusing a Saved Encoder-Decoder Model and Tokenizer in Another Notebook\n",
    "\n",
    "Steps:\n",
    "1) Load your saved tokenizer.\n",
    "2) Construct the same architecture for the encoder-decoder.\n",
    "3) Load the model state_dict from disk.\n",
    "4) Write a simple function to generate the remainder (partial -> remainder).\n",
    "5) Test on a partial snippet to confirm the model works.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1) Imports\n",
    "# ========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BertTokenizer   # or whichever tokenizer you used\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from: C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/encoder-decoder/conversation_model_bert_tokenizer\n",
      "Vocabulary size: 30522\n",
      "Pad token: [PAD] ( 0 )\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 2) Load the Saved Tokenizer\n",
    "# ========================================\n",
    "tokenizer_dir = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/encoder-decoder/conversation_model_bert_tokenizer\"  # or the path you used for saving\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dir) \n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(\"Loaded tokenizer from:\", tokenizer_dir)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Pad token:\", tokenizer.pad_token, \"(\", pad_token_id, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3) Reconstruct the Encoder-Decoder Architecture\n",
    "# ========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        pad_token_id=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out_fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def _generate_causal_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz))==1\n",
    "        mask = mask.transpose(0,1).masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_emb = self.embedding(src)*math.sqrt(self.d_model)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "\n",
    "        dec_emb = self.embedding(tgt)*math.sqrt(self.d_model)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "\n",
    "        src_key_padding_mask = (src==self.pad_token_id)\n",
    "        tgt_key_padding_mask = (tgt==self.pad_token_id)\n",
    "\n",
    "        seq_len_dec = tgt.size(1)\n",
    "        causal_mask = self._generate_causal_mask(seq_len_dec).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=enc_emb,\n",
    "            tgt=dec_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        logits = self.out_fc(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_240\\3114314627.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/encoder-decoder/conversation_model_bert_tokenizer\\model_state.pt\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4) Load the Model State from Disk\n",
    "# ========================================\n",
    "model = TransformerEncoderDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "model_path = os.path.join(tokenizer_dir, \"model_state.pt\")  # e.g., \"model_state.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded from:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5) Inference: Generating the Remainder\n",
    "# ========================================\n",
    "def generate_remainder(model, tokenizer, partial_text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Use a greedy decoding approach.\n",
    "    - partial_text is the input snippet for the encoder.\n",
    "    - We'll start decoder with a single pad or dummy token and\n",
    "      produce next tokens until we hit the pad token or max steps.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # encode partial\n",
    "        enc_partial = tokenizer.encode(\n",
    "            partial_text,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        src = torch.tensor([enc_partial], dtype=torch.long).to(device)\n",
    "\n",
    "        # Start the decoder with [PAD], or define your own BOS approach\n",
    "        dec_in = [tokenizer.pad_token_id]  \n",
    "        dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(src, dec_tensor)\n",
    "            # next token from the last step\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_id = torch.argmax(next_token_logits).item()\n",
    "            dec_in.append(next_id)\n",
    "            dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "            # If we produce PAD again, we can stop, or define a custom EOS\n",
    "            if next_id == tokenizer.pad_token_id:\n",
    "                break\n",
    "\n",
    "        # skip the first token (the dummy \"start\" token)\n",
    "        generated_ids = dec_in[1:]\n",
    "        text_out = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTIAL TEXT ===\n",
      "Hello, I'm Sam. I saw an ad about a photography workshop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "=== PREDICTED REMAINDER ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6) Test on a Partial Snippet\n",
    "# ========================================\n",
    "test_partial = \"Hello, I'm Sam. I saw an ad about a photography workshop.\"\n",
    "print(\"=== PARTIAL TEXT ===\")\n",
    "print(test_partial)\n",
    "\n",
    "predicted_rem = generate_remainder(model, tokenizer, test_partial, max_new_tokens=30)\n",
    "print(\"\\n=== PREDICTED REMAINDER ===\")\n",
    "print(predicted_rem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
