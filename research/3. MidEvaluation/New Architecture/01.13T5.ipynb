{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conversation Prediction Model with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0) Install Requirements (if needed)\n",
    "# ============================================\n",
    "# In a fresh environment or Google Colab, you might need:\n",
    "# !pip install transformers datasets accelerate pandas\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 967\n",
      "   CONVERSATION_ID  CONVERSATION_STEP  \\\n",
      "0                0                  1   \n",
      "1                0                  2   \n",
      "2                0                  3   \n",
      "3                0                  4   \n",
      "4                0                  5   \n",
      "5                0                  6   \n",
      "6                0                  7   \n",
      "7                0                  8   \n",
      "8                0                  9   \n",
      "9                0                 10   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  Hello, this is [Your Name]'s personal assistan...   \n",
      "1  Hi, I'm Sam. I saw an ad about a photography w...   \n",
      "2  Hi Sam, it's great to hear of your interest in...   \n",
      "3  Thanks! I was wondering about the skill level ...   \n",
      "4  The workshop is designed to accommodate all sk...   \n",
      "5  That sounds perfect. What's the registration p...   \n",
      "6  You can register through our website. I can gu...   \n",
      "7  A direct link would be great. Can you also tel...   \n",
      "8  Certainly, the fee for the workshop is $200, w...   \n",
      "9            Sure, it's sam.photography@example.com.   \n",
      "\n",
      "                                    CONTEXT    LABEL Unnamed: 5 Unnamed: 6  \n",
      "0                 Standard opening exchange  neutral        NaN        NaN  \n",
      "1                        Expresses interest  neutral        NaN        NaN  \n",
      "2         Assistant is open and encouraging  neutral        NaN        NaN  \n",
      "3            Addresses the concern directly  neutral        NaN        NaN  \n",
      "4            Addresses the concern directly  neutral        NaN        NaN  \n",
      "5  Directs the conversation to registration  neutral        NaN        NaN  \n",
      "6                 Offers assistance options  neutral        NaN        NaN  \n",
      "7         requesting additional information  neutral        NaN        NaN  \n",
      "8    Proactive in facilitating registration  neutral        NaN        NaN  \n",
      "9                    Provides email address  neutral        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1) Load the CSV Data\n",
    "# ============================================\n",
    "# We'll assume your CSV is named \"conversations.csv\" and has:\n",
    "#   CONVERSATION_ID, CONVERSATION_STEP, TEXT, CONTEXT, LABEL\n",
    "# We only need CONVERSATION_ID, CONVERSATION_STEP, TEXT for building partial->full.\n",
    "\n",
    "df = pd.read_csv(\"/Users/ashansubodha/Desktop/VOIP Vishing/conversation-prediction/FINAL_DATASET2.csv\")\n",
    "print(\"Data size:\", len(df))\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversation pairs: 76\n",
      "                                              source  \\\n",
      "0  Hello, this is [Your Name]'s personal assistan...   \n",
      "1  Hello, this is the personal assistant of [Your...   \n",
      "2  Hello, this is the assistant to [Your Name]. H...   \n",
      "3  Hello, this is the office of [Your Name]. How ...   \n",
      "4  Hello, you've reached the assistant for [Your ...   \n",
      "\n",
      "                                              target  \n",
      "0  Hello, this is [Your Name]'s personal assistan...  \n",
      "1  Hello, this is the personal assistant of [Your...  \n",
      "2  Hello, this is the assistant to [Your Name]. H...  \n",
      "3  Hello, this is the office of [Your Name]. How ...  \n",
      "4  Hello, you've reached the assistant for [Your ...  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2) Build (partial, full) Pairs\n",
    "# ============================================\n",
    "# We'll define a function that:\n",
    "#  1. Groups by CONVERSATION_ID\n",
    "#  2. Sorts by CONVERSATION_STEP\n",
    "#  3. Takes the first 'partial_ratio'% of lines as \"source\"\n",
    "#  4. Takes all lines as \"target\"\n",
    "# This yields a dataset where \"source\" is the partial conversation,\n",
    "# and \"target\" is the entire conversation text.\n",
    "\n",
    "def build_partial_full_pairs(df, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    partial_ratio: fraction of the conversation to treat as 'partial'.\n",
    "                   e.g., 0.5 => first 50% is partial, entire conversation is target.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    grouped = df.groupby(\"CONVERSATION_ID\")\n",
    "    \n",
    "    for convo_id, group in grouped:\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        # Collect all TEXT lines in order\n",
    "        all_texts = group_sorted[\"TEXT\"].tolist()\n",
    "        \n",
    "        # Build the \"full\" conversation by concatenating\n",
    "        full_convo = \"\\n\".join(all_texts)\n",
    "\n",
    "        # Build the \"partial\" by taking first partial_ratio lines\n",
    "        cutoff = max(1, int(len(all_texts) * partial_ratio))  # at least 1 line\n",
    "        partial_texts = all_texts[:cutoff]\n",
    "        partial_convo = \"\\n\".join(partial_texts)\n",
    "\n",
    "        pairs.append({\n",
    "            \"source\": partial_convo,\n",
    "            \"target\": full_convo\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "pairs_df = build_partial_full_pairs(df, partial_ratio=0.5)\n",
    "print(\"Number of conversation pairs:\", len(pairs_df))\n",
    "print(pairs_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, this is [Your Name]'s personal assistan...</td>\n",
       "      <td>Hello, this is [Your Name]'s personal assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello, this is the assistant to [Your Name]. H...</td>\n",
       "      <td>Hello, this is the assistant to [Your Name]. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, this is the office of [Your Name]. How ...</td>\n",
       "      <td>Hello, this is the office of [Your Name]. How ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello, you've reached the assistant for [Your ...</td>\n",
       "      <td>Hello, you've reached the assistant for [Your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "      <td>Hello, this is the personal assistant of [Your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Hello, this is a call from the National Tax Se...</td>\n",
       "      <td>Hello, this is a call from the National Tax Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Hello, is this Mr. OOO?\\nYes, this is him.\\nHi...</td>\n",
       "      <td>Hello, is this Mr. OOO?\\nYes, this is him.\\nHi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>This bond is under your name, not under our ba...</td>\n",
       "      <td>This bond is under your name, not under our ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Hello, this is your manager. I've been trying ...</td>\n",
       "      <td>Hello, this is your manager. I've been trying ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "0   Hello, this is [Your Name]'s personal assistan...   \n",
       "1   Hello, this is the personal assistant of [Your...   \n",
       "2   Hello, this is the assistant to [Your Name]. H...   \n",
       "3   Hello, this is the office of [Your Name]. How ...   \n",
       "4   Hello, you've reached the assistant for [Your ...   \n",
       "..                                                ...   \n",
       "71  Hello, this is the personal assistant of [Your...   \n",
       "72  Hello, this is a call from the National Tax Se...   \n",
       "73  Hello, is this Mr. OOO?\\nYes, this is him.\\nHi...   \n",
       "74  This bond is under your name, not under our ba...   \n",
       "75  Hello, this is your manager. I've been trying ...   \n",
       "\n",
       "                                               target  \n",
       "0   Hello, this is [Your Name]'s personal assistan...  \n",
       "1   Hello, this is the personal assistant of [Your...  \n",
       "2   Hello, this is the assistant to [Your Name]. H...  \n",
       "3   Hello, this is the office of [Your Name]. How ...  \n",
       "4   Hello, you've reached the assistant for [Your ...  \n",
       "..                                                ...  \n",
       "71  Hello, this is the personal assistant of [Your...  \n",
       "72  Hello, this is a call from the National Tax Se...  \n",
       "73  Hello, is this Mr. OOO?\\nYes, this is him.\\nHi...  \n",
       "74  This bond is under your name, not under our ba...  \n",
       "75  Hello, this is your manager. I've been trying ...  \n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60\n",
      "Eval size: 16\n",
      "Sample train example:\n",
      " {'source': \"Hello, this is the assistant to [Your Name]. May I know the reason for your call?\\nHi, I'm calling from 'QuickTech Support' about a critical security issue flagged on [Your Name]'s computer. Immediate action is required.\\nCould you specify what kind of security issue has been detected?\\nWe've detected multiple virus infections that could compromise personal data. We urgently need to run a remote clean-up procedure.\\nBefore we proceed, could you provide me with details on when and how the issue was detected?\\nOur system conducts regular sweeps and your IP address showed up in our latest scan. We're here to help resolve this immediately.\\nI need to cross-verify this with our internal security protocols before any remote access is granted.\", 'target': \"Hello, this is the assistant to [Your Name]. May I know the reason for your call?\\nHi, I'm calling from 'QuickTech Support' about a critical security issue flagged on [Your Name]'s computer. Immediate action is required.\\nCould you specify what kind of security issue has been detected?\\nWe've detected multiple virus infections that could compromise personal data. We urgently need to run a remote clean-up procedure.\\nBefore we proceed, could you provide me with details on when and how the issue was detected?\\nOur system conducts regular sweeps and your IP address showed up in our latest scan. We're here to help resolve this immediately.\\nI need to cross-verify this with our internal security protocols before any remote access is granted.\\nI understand your concern, but we need to act fast to prevent data loss. We're offering this complimentary service as a part of our customer care initiative.\\nWhile we appreciate the assistance, I must first verify the legitimacy of this service to protect against potential unauthorized access.\\nLook, we have limited time. I can provide you with a temporary ID and password for our secure support portal where you can see the report.\\nI will require official documentation of your service and a direct line to your supervisor for confirmation.\\nI don't have the time to wait for supervisors. Your data is at risk now. Let's get this sorted out, shall we?\\nMy priority is to safeguard [Your Name]'s information. Please email the official report and contact details, and we will follow up accordingly.\\nI'll send over what I can, but remember, every minute counts!\\nUnderstood. We will respond as soon as we have conducted our review. Thank you for your patience.\"}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3) Create a Hugging Face Dataset\n",
    "# ============================================\n",
    "dataset = Dataset.from_pandas(pairs_df[[\"source\", \"target\"]])\n",
    "\n",
    "# Let's do a simple train_test_split\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset  = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "print(\"Sample train example:\\n\", train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4) Tokenizer Setup (T5)\n",
    "# ============================================\n",
    "model_name = \"t5-small\"  # or \"t5-base\", \"flan-t5-base\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# T5 typically uses special tokens; we'll rely on T5's own defaults.\n",
    "# If there's a mismatch for pad_token, do:\n",
    "#   tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60 [00:00<?, ? examples/s]/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 911.79 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 934.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train sample: {'input_ids': tensor([ 8774,     6,    48,    19,     8,  6165,    12,   784, 21425,  5570,\n",
      "         4275,   932,    27,   214,     8,  1053,    21,    39,   580,    58,\n",
      "         2018,     6,    27,    31,    51,  3874,    45,     3,    31,  5991,\n",
      "         3142,  9542,  4224,    31,    81,     3,     9,  2404,  1034,   962,\n",
      "         5692,  5402,    30,   784, 21425,  5570,   908,    31,     7,  1218,\n",
      "            5,  1318,  5700,   342,  1041,    19,   831,     5,  9348,    25,\n",
      "        11610,   125,   773,    13,  1034,   962,    65,   118, 14619,    58,\n",
      "          101,    31,   162, 14619,  1317,  6722, 13315,    24,   228, 12326,\n",
      "          525,   331,     5,   101, 10839,   120,   174,    12,   661,     3,\n",
      "            9,  4322,  1349,    18,   413,  3979,     5,  3103,    62,  8669,\n",
      "            6,   228,    25,   370,   140,    28,  1030,    30,   116,    11,\n",
      "          149,     8,   962,    47, 14619,    58,   421,   358,  3498,     7,\n",
      "         1646, 17695,     7,    11,    39,  3857,  1115,  3217,    95,    16,\n",
      "           69,  1251,  5924,     5,   101,    31,    60,   270,    12,   199,\n",
      "         7785,    48,  2017,     5,    27,   174,    12,  2269,    18,   624,\n",
      "         4921,    48,    28,    69,  3224,  1034, 18870,   274,   136,  4322,\n",
      "          592,    19,  7020,     5,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([ 8774,     6,    48,    19,     8,  6165,    12,   784, 21425,  5570,\n",
      "         4275,   932,    27,   214,     8,  1053,    21,    39,   580,    58,\n",
      "         2018,     6,    27,    31,    51,  3874,    45,     3,    31,  5991,\n",
      "         3142,  9542,  4224,    31,    81,     3,     9,  2404,  1034,   962,\n",
      "         5692,  5402,    30,   784, 21425,  5570,   908,    31,     7,  1218,\n",
      "            5,  1318,  5700,   342,  1041,    19,   831,     5,  9348,    25,\n",
      "        11610,   125,   773,    13,  1034,   962,    65,   118, 14619,    58,\n",
      "          101,    31,   162, 14619,  1317,  6722, 13315,    24,   228, 12326,\n",
      "          525,   331,     5,   101, 10839,   120,   174,    12,   661,     3,\n",
      "            9,  4322,  1349,    18,   413,  3979,     5,  3103,    62,  8669,\n",
      "            6,   228,    25,   370,   140,    28,  1030,    30,   116,    11,\n",
      "          149,     8,   962,    47, 14619,    58,   421,   358,  3498,     7,\n",
      "         1646, 17695,     7,    11,    39,  3857,  1115,  3217,    95,    16,\n",
      "           69,  1251,  5924,     5,   101,    31,    60,   270,    12,   199,\n",
      "         7785,    48,  2017,     5,    27,   174,    12,  2269,    18,   624,\n",
      "         4921,    48,    28,    69,  3224,  1034, 18870,   274,   136,  4322,\n",
      "          592,    19,  7020,     5,    27,   734,    39,  2410,     6,    68,\n",
      "           62,   174,    12,  1810,  1006,    12,  1709,   331,  1453,     5,\n",
      "          101,    31,    60,  1772,    48, 13968,   313,    38,     3,     9,\n",
      "          294,    13,    69,   884,   124,  6121,     5,   818,    62,  3653,\n",
      "            8,  2927,     6,    27,   398,   166, 10446,     8, 31841,    13,\n",
      "           48,   313,    12,  1822,   581,  1055,     3, 22556,   592,     5,\n",
      "         3568,     6,    62,    43,  1643,    97,     5,    27,    54,   370,\n",
      "           25,    28,     3,     9,  7234,  4699,    11,  4735,    21,    69,\n",
      "         2451,   380,  8948,   213,    25,    54,   217,     8,   934,     5,\n",
      "           27,    56,  1457,  2314,  7192,     1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5) Preprocessing Function\n",
    "# ============================================\n",
    "# We treat \"source\" as the input (encoder) and \"target\" as the output (decoder).\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Encode source\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    # Encode target\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset  = eval_dataset.map(preprocess_function,  batched=True)\n",
    "\n",
    "# Remove original columns to keep only the tokenized fields\n",
    "train_dataset = train_dataset.remove_columns([\"source\",\"target\"])\n",
    "eval_dataset  = eval_dataset.remove_columns([\"source\",\"target\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Processed train sample:\", train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6) Data Collator for Seq2Seq\n",
    "# ============================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_name,\n",
    "    padding=\"longest\",  # or \"max_length\"\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7) Load T5 Model\n",
    "# ============================================\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=t5-conversation-prediction/runs/Jan13_17-35-13_192.168.1.5,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=t5-conversation-prediction,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=t5-conversation-prediction,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8) Training Arguments\n",
    "# ============================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5-conversation-prediction\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,       # adjust for real data\n",
    "    per_device_train_batch_size=2,  # adjust\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9) Define Trainer\n",
    "# ============================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "                                               \n",
      " 33%|███▎      | 30/90 [00:20<00:23,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.965167760848999, 'eval_runtime': 1.5266, 'eval_samples_per_second': 10.481, 'eval_steps_per_second': 5.241, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [00:28<00:14,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8344, 'grad_norm': 2.227100372314453, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 60/90 [00:32<00:09,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.768527626991272, 'eval_runtime': 0.5225, 'eval_samples_per_second': 30.625, 'eval_steps_per_second': 15.312, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 90/90 [00:45<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.727165937423706, 'eval_runtime': 0.4735, 'eval_samples_per_second': 33.791, 'eval_steps_per_second': 16.896, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 90/90 [00:46<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.7162, 'train_samples_per_second': 3.853, 'train_steps_per_second': 1.927, 'train_loss': 2.470164320203993, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('t5-conversation-prediction/tokenizer_config.json',\n",
       " 't5-conversation-prediction/special_tokens_map.json',\n",
       " 't5-conversation-prediction/spiece.model',\n",
       " 't5-conversation-prediction/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10) Train the Model\n",
    "# ============================================\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(\"t5-conversation-prediction\")\n",
    "tokenizer.save_pretrained(\"t5-conversation-prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Conversation ===\n",
      " Good Morning, I am Sanuja calling on behalf of State Bank of Sri Lanka. Oh, hi. I'm actually in a meeting right now. Could you call later?\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 11) Generate (Inference)\n",
    "# ============================================\n",
    "# We'll define a helper function that, given a partial conversation,\n",
    "# uses the fine-tuned T5 to generate the rest.\n",
    "\n",
    "def predict_conversation(partial_convo, max_new_tokens=100):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        partial_convo,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_new_tokens,\n",
    "        num_beams=4,       # or do_sample=True for sampling\n",
    "        early_stopping=True\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "partial_conversation = \"\"\"Good Morning, I am Sanuja calling on behalf of State Bank of Sri Lanka. Oh, hi. I'm actually in a meeting right now. Could you call later?\n",
    "\"\"\"\n",
    "completion = predict_conversation(partial_conversation, max_new_tokens=300)\n",
    "print(\"=== Generated Conversation ===\\n\", completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voipvishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
