{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Turn-Level Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0) Install Requirements (if necessary)\n",
    "# ============================================\n",
    "# In a new environment or Google Colab, you might need:\n",
    "!pip install transformers datasets accelerate pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1) Imports\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Number of rows: 967\n",
      "   CONVERSATION_ID  CONVERSATION_STEP  \\\n",
      "0                0                  1   \n",
      "1                0                  2   \n",
      "2                0                  3   \n",
      "3                0                  4   \n",
      "4                0                  5   \n",
      "5                0                  6   \n",
      "6                0                  7   \n",
      "7                0                  8   \n",
      "8                0                  9   \n",
      "9                0                 10   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  Hello, this is [Your Name]'s personal assistan...   \n",
      "1  Hi, I'm Sam. I saw an ad about a photography w...   \n",
      "2  Hi Sam, it's great to hear of your interest in...   \n",
      "3  Thanks! I was wondering about the skill level ...   \n",
      "4  The workshop is designed to accommodate all sk...   \n",
      "5  That sounds perfect. What's the registration p...   \n",
      "6  You can register through our website. I can gu...   \n",
      "7  A direct link would be great. Can you also tel...   \n",
      "8  Certainly, the fee for the workshop is $200, w...   \n",
      "9            Sure, it's sam.photography@example.com.   \n",
      "\n",
      "                                    CONTEXT    LABEL Unnamed: 5 Unnamed: 6  \n",
      "0                 Standard opening exchange  neutral        NaN        NaN  \n",
      "1                        Expresses interest  neutral        NaN        NaN  \n",
      "2         Assistant is open and encouraging  neutral        NaN        NaN  \n",
      "3            Addresses the concern directly  neutral        NaN        NaN  \n",
      "4            Addresses the concern directly  neutral        NaN        NaN  \n",
      "5  Directs the conversation to registration  neutral        NaN        NaN  \n",
      "6                 Offers assistance options  neutral        NaN        NaN  \n",
      "7         requesting additional information  neutral        NaN        NaN  \n",
      "8    Proactive in facilitating registration  neutral        NaN        NaN  \n",
      "9                    Provides email address  neutral        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2) Load the CSV dataset\n",
    "# ============================================\n",
    "# We'll assume you saved the table of data to \"dialogues.csv\".\n",
    "# The table has columns:\n",
    "#   CONVERSATION_ID | CONVERSATION_STEP | TEXT | CONTEXT | LABEL\n",
    "\n",
    "df = pd.read_csv(\"/Users/ashansubodha/Desktop/VOIP Vishing/conversation-prediction/FINAL_DATASET2.csv\")\n",
    "print(\"Data loaded. Number of rows:\", len(df))\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['neutral' 'slightly_suspicious' 'legitimate' 'potential_scam'\n",
      " 'suspicious' 'highly_suspicious' ' neutral' 'scam' 'scam_response'\n",
      " ' scam_response' ' potential_scam' ' legitimate' ' scam'\n",
      " ' dismissing official protocols' ' emphasizing security and compliance'\n",
      " ' ready for further engagement' ' adhering to protocols'\n",
      " ' Expressing admiration' ' even winning gold!\"'\n",
      " ' Describing early experience' ' Describing inline skating'\n",
      " ' Reflecting on the difficulty' ' but then they started catching up.\"'\n",
      " ' Expressing emotional reaction' ' Reflecting on perseverance'\n",
      " ' Reflecting on Korean resilience'\n",
      " ' winter sportsâ€”theyâ€™re consistently at the top.\"'\n",
      " ' and I always thought I would be good at it.\"'\n",
      " ' Reflecting on personal experience' \" Praising Korea's sports prowess\"\n",
      " ' Concluding statement']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3) Label Inspection\n",
    "# ============================================\n",
    "# Let's see what unique labels exist in the 'LABEL' column.\n",
    "unique_labels = df[\"LABEL\"].unique()\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "# Example output might be:\n",
    "# ['neutral', 'slightly_suspicious', 'suspicious', 'potential_scam', 'legitimate']\n",
    "# or something similar depending on your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {' Concluding statement': 0, ' Describing early experience': 1, ' Describing inline skating': 2, ' Expressing admiration': 3, ' Expressing emotional reaction': 4, \" Praising Korea's sports prowess\": 5, ' Reflecting on Korean resilience': 6, ' Reflecting on perseverance': 7, ' Reflecting on personal experience': 8, ' Reflecting on the difficulty': 9, ' adhering to protocols': 10, ' and I always thought I would be good at it.\"': 11, ' but then they started catching up.\"': 12, ' dismissing official protocols': 13, ' emphasizing security and compliance': 14, ' even winning gold!\"': 15, ' legitimate': 16, ' neutral': 17, ' potential_scam': 18, ' ready for further engagement': 19, ' scam': 20, ' scam_response': 21, ' winter sportsâ€”theyâ€™re consistently at the top.\"': 22, 'highly_suspicious': 23, 'legitimate': 24, 'neutral': 25, 'potential_scam': 26, 'scam': 27, 'scam_response': 28, 'slightly_suspicious': 29, 'suspicious': 30}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4) Prepare the Data for Turn-Level Classification\n",
    "# ============================================\n",
    "# We'll define a simple classification task: each row is one example,\n",
    "# input = TEXT, label = LABEL.\n",
    "\n",
    "# Some labels might be \"neutral\", \"potential_scam\", \"suspicious\", etc.\n",
    "# We need to map them to integer IDs for the model.\n",
    "\n",
    "label_list = list(unique_labels)\n",
    "label_list.sort()  # optional, to keep them in a deterministic order\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(\"label2id:\", label2id)\n",
    "\n",
    "# We'll add a numeric label column to the DataFrame\n",
    "df[\"numeric_label\"] = df[\"LABEL\"].apply(lambda x: label2id[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 773 Test size: 194\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5) Train-Test Split\n",
    "# ============================================\n",
    "# We'll do a simple 80-20 split at the row-level. \n",
    "# If you need conversation-level splits (i.e. no conversation is partially in train/test),\n",
    "# you'll do that by grouping on CONVERSATION_ID. \n",
    "# For now, let's keep it simple.\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example: {'TEXT': 'While we understand the urgency, we must adhere to our internal protocols for legal matters. All requests of this nature need to be documented and reviewed by our legal team.', 'labels': 21, '__index_level_0__': 720}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6) Convert to Hugging Face Dataset\n",
    "# ============================================\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"TEXT\", \"numeric_label\"]])\n",
    "test_dataset  = Dataset.from_pandas(test_df[[\"TEXT\", \"numeric_label\"]])\n",
    "\n",
    "# rename \"numeric_label\" to \"labels\" so Trainer knows it's the label\n",
    "train_dataset = train_dataset.rename_column(\"numeric_label\", \"labels\")\n",
    "test_dataset  = test_dataset.rename_column(\"numeric_label\", \"labels\")\n",
    "\n",
    "print(\"Train example:\", train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 773/773 [00:00<00:00, 3776.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:00<00:00, 4434.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample after tokenization: {'labels': tensor(21), '__index_level_0__': tensor(720), 'input_ids': tensor([  101,  2096,  2057,  3305,  1996, 19353,  1010,  2057,  2442, 25276,\n",
      "         2000,  2256,  4722, 16744,  2005,  3423,  5609,  1012,  2035, 11186,\n",
      "         1997,  2023,  3267,  2342,  2000,  2022,  8832,  1998,  8182,  2011,\n",
      "         2256,  3423,  2136,  1012,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Tokenization\n",
    "# ============================================\n",
    "# We'll load a BERT tokenizer (could be \"bert-base-uncased\" or any variant).\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"TEXT\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize_function,  batched=True)\n",
    "\n",
    "# Remove the original text column to keep only tokenized fields\n",
    "train_dataset = train_dataset.remove_columns([\"TEXT\"])\n",
    "test_dataset  = test_dataset.remove_columns([\"TEXT\"])\n",
    "\n",
    "# set format to PyTorch\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Train sample after tokenization:\", train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8) Load the Model for Sequence Classification\n",
    "# ============================================\n",
    "# We'll define the number of labels as the length of label_list.\n",
    "\n",
    "num_labels = len(label_list)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# We'll also define a custom compute_metrics function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/voipvishing/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2t/7vrwz9ms6c1b8_81nddnvzkr0000gn/T/ipykernel_39081/2405708361.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 9) Define Training Arguments\n",
    "# ============================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"turn_classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "# We'll use the Trainer API from Hugging Face\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–Š         | 50/582 [00:14<01:57,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6102, 'grad_norm': 7.814505577087402, 'learning_rate': 4.570446735395189e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 100/582 [00:25<01:48,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0148, 'grad_norm': 9.731621742248535, 'learning_rate': 4.140893470790378e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 150/582 [00:37<01:37,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7797, 'grad_norm': 24.606828689575195, 'learning_rate': 3.7113402061855674e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/582 [00:53<07:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.780285120010376, 'eval_accuracy': 0.4484536082474227, 'eval_f1': 0.3686819429877295, 'eval_runtime': 3.0667, 'eval_samples_per_second': 63.261, 'eval_steps_per_second': 15.978, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 200/582 [00:56<03:44,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8361, 'grad_norm': 10.021393775939941, 'learning_rate': 3.2817869415807564e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/582 [01:07<01:14,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4984, 'grad_norm': 27.179426193237305, 'learning_rate': 2.852233676975945e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/582 [01:18<01:02,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5212, 'grad_norm': 11.409460067749023, 'learning_rate': 2.422680412371134e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 350/582 [01:30<00:51,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2466, 'grad_norm': 8.256134033203125, 'learning_rate': 1.9931271477663232e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/582 [01:40<00:38,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5115593671798706, 'eval_accuracy': 0.5721649484536082, 'eval_f1': 0.4911569189178284, 'eval_runtime': 2.618, 'eval_samples_per_second': 74.102, 'eval_steps_per_second': 18.716, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/582 [01:45<00:45,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3203, 'grad_norm': 11.056227684020996, 'learning_rate': 1.5635738831615122e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/582 [01:56<00:29,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2439, 'grad_norm': 8.876267433166504, 'learning_rate': 1.134020618556701e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/582 [02:07<00:18,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0048, 'grad_norm': 8.128521919250488, 'learning_rate': 7.0446735395189e-06, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/582 [02:18<00:07,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0282, 'grad_norm': 11.993239402770996, 'learning_rate': 2.7491408934707903e-06, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 582/582 [02:30<00:00,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4126800298690796, 'eval_accuracy': 0.5721649484536082, 'eval_f1': 0.5082952163815565, 'eval_runtime': 2.6158, 'eval_samples_per_second': 74.164, 'eval_steps_per_second': 18.732, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 582/582 [02:31<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 151.8442, 'train_samples_per_second': 15.272, 'train_steps_per_second': 3.833, 'train_loss': 1.5335614968001638, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=582, training_loss=1.5335614968001638, metrics={'train_runtime': 151.8442, 'train_samples_per_second': 15.272, 'train_steps_per_second': 3.833, 'total_flos': 152578352247552.0, 'train_loss': 1.5335614968001638, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10) Train the Model\n",
    "# ============================================\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:02<00:00, 18.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.4126800298690796, 'eval_accuracy': 0.5721649484536082, 'eval_f1': 0.5082952163815565, 'eval_runtime': 3.003, 'eval_samples_per_second': 64.601, 'eval_steps_per_second': 16.317, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 11) Evaluate on the Test Set\n",
    "# ============================================\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def classify_utterance(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    # Move inputs to device:\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # model is already on device\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = logits.argmax(dim=-1).item()\n",
    "    return id2label[predicted_class_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: Hello, I'm calling about your bank account and need urgent details.\n",
      "Predicted Label: scam\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, I'm calling about your bank account and need urgent details.\"\n",
    "predicted_label = classify_utterance(sample_text)\n",
    "print(f\"Sample Text: {sample_text}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voipvishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
