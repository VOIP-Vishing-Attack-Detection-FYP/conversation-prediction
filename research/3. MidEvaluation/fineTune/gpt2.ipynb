{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nJupyter Notebook:\\nFine-Tuning GPT-2 for Conversation Prediction (Partial -> Remainder).\\n\\nWe will:\\n1) Load a CSV with columns: CONVERSATION_ID, CONVERSATION_STEP, TEXT\\n2) Build partial->remainder pairs for each conversation\\n3) Create a single text = partial + <SEP> + remainder\\n4) Train GPT-2 (causal LM) on these examples\\n5) Save the fine-tuned model & tokenizer for reuse\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter Notebook:\n",
    "Fine-Tuning GPT-2 for Conversation Prediction (Partial -> Remainder).\n",
    "\n",
    "We will:\n",
    "1) Load a CSV with columns: CONVERSATION_ID, CONVERSATION_STEP, TEXT\n",
    "2) Build partial->remainder pairs for each conversation\n",
    "3) Create a single text = partial + <SEP> + remainder\n",
    "4) Train GPT-2 (causal LM) on these examples\n",
    "5) Save the fine-tuned model & tokenizer for reuse\n",
    "\"\"\"\n",
    "\n",
    "# If needed, install:\n",
    "# !pip install transformers datasets accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1) Imports\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math, os\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Sample pair:\n",
      "Partial: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? \n",
      "Remainder: You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee?\n",
      "Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com.\n",
      "Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help!\n",
      "You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 2) Build partial->remainder from CSV\n",
    "# =========================================\n",
    "def build_partial_remainder(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    partial_ratio <--- param to tune how many lines go into partial snippet\n",
    "    e.g. partial_ratio=0.5 means first 50% lines of the conversation = partial,\n",
    "                              last 50% = remainder\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "        cutoff = max(1, int(len(texts)*partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list = texts[cutoff:]\n",
    "        partial_str = \"\\n\".join(partial_list).strip()\n",
    "        remainder_str = \"\\n\".join(remainder_list).strip() if remainder_list else \"\"\n",
    "        if partial_str and remainder_str:\n",
    "            pairs.append((partial_str, remainder_str))\n",
    "    return pairs\n",
    "\n",
    "# Example usage (update the path):\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  \n",
    "partial_ratio = 0.5         # <--- param to tune\n",
    "data_pairs = build_partial_remainder(csv_path, partial_ratio)\n",
    "print(\"Number of partial->remainder pairs:\", len(data_pairs))\n",
    "if data_pairs:\n",
    "    print(\"Sample pair:\\nPartial:\", data_pairs[0][0], \"\\nRemainder:\", data_pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3) Create a single text with <|SEP|>\n",
    "# =========================================\n",
    "# We'll combine partial + special sep + remainder into one string.\n",
    "# GPT-2 will treat partial snippet as context, and remainder as the next tokens to predict.\n",
    "\n",
    "def make_single_text(partial, remainder, sep_token=\"<|SEP|>\"):\n",
    "    \"\"\"\n",
    "    partial + sep_token + remainder\n",
    "    e.g. \"Hello.\\nCallee: I saw your ad.<|SEP|>This is the remainder...\"\n",
    "    \"\"\"\n",
    "    return f\"{partial}\\n{sep_token}\\n{remainder}\"\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    We'll store the final strings. Each item is a single text line\n",
    "    that includes partial+SEP+remainder for GPT-2.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, sep_token=\"<|SEP|>\"):\n",
    "        self.texts = []\n",
    "        for (part, rem) in pairs:\n",
    "            combined = make_single_text(part, rem, sep_token=sep_token)\n",
    "            self.texts.append(combined)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <|SEP|> to GPT-2 tokenizer vocab.\n",
      "Vocabulary size: 50258\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 4) GPT-2 Tokenizer\n",
    "# =========================================\n",
    "# We'll add a custom <SEP> token if we want GPT-2 to treat it specially.\n",
    "# If you want to treat <|SEP|> as just normal text, skip special token addition.\n",
    "# We'll do it as a new token for clarity.\n",
    "\n",
    "model_name = \"gpt2\"  # <--- param: choose \"gpt2-medium\", \"gpt2-large\" if you have bigger GPU\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We can add \"<|SEP|>\" as a special token\n",
    "special_tokens = {\"sep_token\": \"<|SEP|>\"}\n",
    "if \"<|SEP|>\" not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|SEP|>\"]})\n",
    "    print(\"Added <|SEP|> to GPT-2 tokenizer vocab.\")\n",
    "    \n",
    "# GPT-2 doesn't have a real pad token, so let's set pad to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Vocabulary size:\", len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 76\n",
      "Example text: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process?\n",
      "<|SEP|>\n",
      "You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee?\n",
      "Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com.\n",
      "Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help!\n",
      "You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5) Build the Dataset\n",
    "# =========================================\n",
    "ds = ConversationDataset(data_pairs, sep_token=\"<|SEP|>\")\n",
    "print(\"Dataset length:\", len(ds))\n",
    "if len(ds)>0:\n",
    "    print(\"Example text:\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 68\n",
      "Eval size : 8\n"
     ]
    }
   ],
   "source": [
    "# We'll do a train/val split\n",
    "train_size = int(0.9 * len(ds))  # <--- param to tune train vs val ratio\n",
    "eval_size  = len(ds) - train_size\n",
    "train_ds, eval_ds = torch.utils.data.random_split(ds, [train_size, eval_size])\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Eval size :\", len(eval_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6) Data Collator for Causal LM\n",
    "# =========================================\n",
    "# We'll tokenize inside the collator, then do standard next-token prediction ignoring pad.\n",
    "\n",
    "class ConversationDataCollator:\n",
    "    def __init__(self, tokenizer, max_length=128):  # <--- param: max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # 'examples' is a list of strings from our dataset\n",
    "        encoding = self.tokenizer(\n",
    "            examples,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # For GPT-2, input_ids are also labels for causal LM\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "        return encoding\n",
    "\n",
    "max_length = 128  # <--- param: you can tune\n",
    "collator = ConversationDataCollator(tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 128])\n",
      "attention_mask torch.Size([2, 128])\n",
      "labels torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "# Quick check with one batch\n",
    "example_batch = [ds[0], ds[1]]\n",
    "encoded = collator(example_batch)\n",
    "for k,v in encoded.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 68\n",
      "}) Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 8\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 7) Create DataLoaders or Use Trainer Directly\n",
    "# =========================================\n",
    "# We'll rely on Hugging Face Trainer, so we define HuggingFace Datasets\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "train_texts = [train_ds[i] for i in range(len(train_ds))]\n",
    "eval_texts  = [eval_ds[i]  for i in range(len(eval_ds))]\n",
    "\n",
    "train_hf = HFDataset.from_dict({\"text\": train_texts})\n",
    "eval_hf  = HFDataset.from_dict({\"text\": eval_texts})\n",
    "\n",
    "print(train_hf, eval_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 68/68 [00:00<00:00, 187.87 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 151.41 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 4452.00 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2333.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# We'll define a tokenize function for usage with map\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "train_hf = train_hf.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "eval_hf  = eval_hf.map(tokenize_function,  batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "train_hf = train_hf.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n",
    "eval_hf  = eval_hf.map(lambda x: {\"labels\": x[\"input_ids\"]},  batched=True)\n",
    "\n",
    "train_hf.set_format(\"torch\")\n",
    "eval_hf.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# 8) Load GPT-2 Model\n",
    "# =========================================\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# If we added special tokens (like <|SEP|>), we should resize embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 9) Training Arguments\n",
    "# =========================================\n",
    "num_epochs = 2       \n",
    "train_batch_size = 2  \n",
    "lr = 5e-5             \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2_conversation_predict\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4248\\258246823.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 15%|█▍        | 10/68 [00:39<02:53,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0631, 'grad_norm': 13.99717903137207, 'learning_rate': 4.2647058823529415e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/68 [01:07<02:09,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8709, 'grad_norm': 14.305420875549316, 'learning_rate': 3.529411764705883e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 30/68 [01:34<01:41,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.555, 'grad_norm': 13.082621574401855, 'learning_rate': 2.7941176470588236e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|█████     | 34/68 [01:47<01:31,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6475071907043457, 'eval_runtime': 1.9891, 'eval_samples_per_second': 4.022, 'eval_steps_per_second': 2.011, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 40/68 [02:07<01:24,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3386, 'grad_norm': 15.284363746643066, 'learning_rate': 2.058823529411765e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 50/68 [02:34<00:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2335, 'grad_norm': 12.967528343200684, 'learning_rate': 1.323529411764706e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 60/68 [03:00<00:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1923, 'grad_norm': 14.030877113342285, 'learning_rate': 5.882352941176471e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 68/68 [03:26<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5611648559570312, 'eval_runtime': 2.0186, 'eval_samples_per_second': 3.963, 'eval_steps_per_second': 1.982, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:29<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 209.4546, 'train_samples_per_second': 0.649, 'train_steps_per_second': 0.325, 'train_loss': 2.490679151871625, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=68, training_loss=2.490679151871625, metrics={'train_runtime': 209.4546, 'train_samples_per_second': 0.649, 'train_steps_per_second': 0.325, 'total_flos': 8883929088000.0, 'train_loss': 2.490679151871625, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# 10) Trainer\n",
    "# =========================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf,\n",
    "    eval_dataset=eval_hf,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator  # or you can pass the custom collator if you want\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + tokenizer saved to gpt2_conversation_predict_model\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 11) Save the Finetuned Model + Tokenizer\n",
    "# =========================================\n",
    "save_dir = \"gpt2_conversation_predict_model\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Model + tokenizer saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: Hello, I'm Sam. I saw an ad about a photography workshop.\n",
      "Completion: Hello, I'm Sam. I saw an ad about a photography workshop. How can I help you?\n",
      "I'm calling from the photography workshop. We're offering a free workshop to help you improve your photography skills.\n",
      "I'm sure you've heard about the workshop. How can I help you?\n",
      "I'm calling\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_dir = \"gpt2_conversation_predict_model\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "prompt_partial = \"Hello, I'm Sam. I saw an ad about a photography workshop.\"\n",
    "inputs = tokenizer(prompt_partial, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False \n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Partial:\", prompt_partial)\n",
    "print(\"Completion:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
