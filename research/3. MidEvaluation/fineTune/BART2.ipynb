{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFine-Tuning BART so that partial -> entire conversation (not just remainder)\\n\\nSteps:\\n1) Build partial->entire pairs from your CSV\\n2) Convert them into a Hugging Face Dataset (source=partial, target=entire)\\n3) Tokenize with BartTokenizer\\n4) Fine-tune BartForConditionalGeneration\\n5) Save the model for reuse\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-Tuning BART so that partial -> entire conversation (not just remainder)\n",
    "\n",
    "Steps:\n",
    "1) Build partial->entire pairs from your CSV\n",
    "2) Convert them into a Hugging Face Dataset (source=partial, target=entire)\n",
    "3) Tokenize with BartTokenizer\n",
    "4) Fine-tune BartForConditionalGeneration\n",
    "5) Save the model for reuse\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1) Imports\n",
    "# ======================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizer, \n",
    "    BartForConditionalGeneration, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->entire pairs: 76\n",
      "\n",
      "Sample pair:\n",
      "Partial: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? \n",
      "Entire: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process?\n",
      "You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee?\n",
      "Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com.\n",
      "Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help!\n",
      "You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 2) Build partial->entire from CSV\n",
    "# ======================================\n",
    "def build_partial_entire(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    For each conversation, \n",
    "      partial snippet = first partial_ratio lines,\n",
    "      entire snippet  = all lines in that conversation.\n",
    "    Return list of (partial_str, entire_str).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        lines = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(lines) < 2:\n",
    "            continue\n",
    "\n",
    "        # entire conversation\n",
    "        entire_str = \"\\n\".join(lines).strip()\n",
    "\n",
    "        # partial snippet\n",
    "        cutoff = max(1, int(len(lines)*partial_ratio))\n",
    "        partial_lines = lines[:cutoff]\n",
    "        partial_str = \"\\n\".join(partial_lines).strip()\n",
    "\n",
    "        if partial_str and entire_str:\n",
    "            pairs.append((partial_str, entire_str))\n",
    "    return pairs\n",
    "\n",
    "# Example usage\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  \n",
    "partial_ratio = 0.5\n",
    "pairs = build_partial_entire(csv_path, partial_ratio)\n",
    "print(\"Number of partial->entire pairs:\", len(pairs))\n",
    "if pairs:\n",
    "    print(\"\\nSample pair:\\nPartial:\", pairs[0][0], \"\\nEntire:\", pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 76\n",
      "})\n",
      "\n",
      "Sample record: {'source': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'target': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\\nYou can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 3) Build a HF Dataset\n",
    "# ======================================\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def create_hf_dataset(pairs):\n",
    "    data_dict = {\n",
    "        \"source\": [p[0] for p in pairs],\n",
    "        \"target\": [p[1] for p in pairs],\n",
    "    }\n",
    "    hf_ds = HFDataset.from_dict(data_dict)\n",
    "    return hf_ds\n",
    "\n",
    "hf_ds = create_hf_dataset(pairs)\n",
    "print(hf_ds)\n",
    "if len(hf_ds)>0:\n",
    "    print(\"\\nSample record:\", hf_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 68 Val size: 8\n"
     ]
    }
   ],
   "source": [
    "# We'll do a train/val split\n",
    "train_size = int(0.9 * len(hf_ds))\n",
    "val_size   = len(hf_ds) - train_size\n",
    "hf_train, hf_val = hf_ds.train_test_split(test_size=val_size).values()\n",
    "\n",
    "print(\"Train size:\", len(hf_train), \"Val size:\", len(hf_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BART model + tokenizer: facebook/bart-base\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 4) Load BART\n",
    "# ======================================\n",
    "model_name = \"facebook/bart-base\"  # or bart-large if you have bigger GPU\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)\n",
    "print(\"Loaded BART model + tokenizer:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/68 [00:00<?, ? examples/s]c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 165.18 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 209.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample after tokenization: {'input_ids': tensor([    0, 31414,     6,    42,    16,   646, 12861, 10704, 46117,    29,\n",
      "         3167,     4,  1336,    64,    38,  3991,    47,   452,   116, 50118,\n",
      "        30086,   328, 24953,     6,   646, 12861, 10704,   742,    34,    57,\n",
      "         3919,   734, 50118, 10836,    47,   694,    55,  1254,    59,     5,\n",
      "         3096,   646, 12861, 10704,   742,  2867,     7,   339,    42,  4588,\n",
      "          116, 50118, 10643,   768,     6,    24,    21,    10,  9624,  4230,\n",
      "          734, 50118, 35299,    47, 17151,    61, 13778,  3595,     8,     5,\n",
      "         6089,  1110,     9,    42,  3096,   116, 50118,   243,  1171,   484,\n",
      "          299,    12, 15512,  3595,   734, 50118,  2709, 14925,  6216,     6,\n",
      "          189,    38,    33,    10,  5135,   346,    50,   781, 12059,    59,\n",
      "            5,  4588,   116,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([    0, 31414,     6,    42,    16,   646, 12861, 10704, 46117,    29,\n",
      "         3167,     4,  1336,    64,    38,  3991,    47,   452,   116, 50118,\n",
      "        30086,   328, 24953,     6,   646, 12861, 10704,   742,    34,    57,\n",
      "         3919,   734, 50118, 10836,    47,   694,    55,  1254,    59,     5,\n",
      "         3096,   646, 12861, 10704,   742,  2867,     7,   339,    42,  4588,\n",
      "          116, 50118, 10643,   768,     6,    24,    21,    10,  9624,  4230,\n",
      "          734, 50118, 35299,    47, 17151,    61, 13778,  3595,     8,     5,\n",
      "         6089,  1110,     9,    42,  3096,   116, 50118,   243,  1171,   484,\n",
      "          299,    12, 15512,  3595,   734, 50118,  2709, 14925,  6216,     6,\n",
      "          189,    38,    33,    10,  5135,   346,    50,   781, 12059,    59,\n",
      "            5,  4588,   116, 50118,   170,  1051,    66, 16926,  1241,  1047,\n",
      "          734, 50118, 17206,    52,  9073,     6,    38,   581,   240,     7,\n",
      "        12881,    42,  4588,    19,    10,   371,    12,  6493,  2267,  2591,\n",
      "         1218,     4, 50118,   170,  1346,   110,  8038,     6,    53, 20607,\n",
      "          328,   734, 50118,   243,    18,    84,   714,     7,  2883, 10675,\n",
      "        14925,   137,  8394,   143, 10761,    50,  1523,     4,  2615,    47,\n",
      "          694,    10,  1511,    23,   110,  1651,    13,   617, 14925,   116,\n",
      "        50118, 32541,     6,    47,    64,  1338,    84,  4588,  3854,  1494,\n",
      "          734, 50118,   100,    40,    28,    11,  2842,    19,   110,  4588,\n",
      "         3854,  1494,  3691,     4,  2276,     6,  2540,  2142,    70,  4249,\n",
      "        14877,     8,   781,  1110,     9,     5,  4588,     7,    84,   558,\n",
      "         1047,     4, 50118, 17245, 31935,     6,    53,  2145,     6,    86,\n",
      "           16,     9,     5, 14981,   734, 50118, 13987,    47,    13,     5,\n",
      "          335,     4,   166,    40,  1551,     5, 14877,  2115, 18245,     8,\n",
      "         1407,    62, 14649,     4,     2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 5) Tokenization\n",
    "# ======================================\n",
    "def tokenize_fn(examples):\n",
    "    # \"source\" => partial snippet\n",
    "    # \"target\" => entire conversation\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=128,   # <--- param: tune\n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            max_length=256, # <--- param: tune (entire might be longer)\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "hf_train = hf_train.map(tokenize_fn, batched=True, remove_columns=[\"source\",\"target\"])\n",
    "hf_val   = hf_val.map(tokenize_fn,   batched=True, remove_columns=[\"source\",\"target\"])\n",
    "\n",
    "hf_train.set_format(\"torch\")\n",
    "hf_val.set_format(\"torch\")\n",
    "\n",
    "print(\"Train sample after tokenization:\", hf_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 6) DataCollatorForSeq2Seq\n",
    "# ======================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11460\\97343872.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 7) Trainer Setup\n",
    "# ======================================\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"bart_partial_entire_convo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,           # <--- param: tune\n",
    "    per_device_train_batch_size=2,# <--- param: tune\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,           # <--- param: tune\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 56/102 [24:11<19:52, 25.92s/it]\n",
      " 10%|▉         | 10/102 [00:48<07:05,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1097, 'grad_norm': 4.584842681884766, 'learning_rate': 4.5098039215686275e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 20/102 [01:42<07:31,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0704, 'grad_norm': 4.315908432006836, 'learning_rate': 4.0196078431372555e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/102 [02:28<05:14,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0973, 'grad_norm': 4.7568583488464355, 'learning_rate': 3.529411764705883e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|███▎      | 34/102 [02:49<04:53,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2899765968322754, 'eval_runtime': 3.7778, 'eval_samples_per_second': 2.118, 'eval_steps_per_second': 1.059, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 40/102 [03:17<04:44,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0201, 'grad_norm': 4.267385005950928, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 50/102 [04:01<03:49,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9144, 'grad_norm': 4.710136890411377, 'learning_rate': 2.5490196078431373e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 60/102 [04:48<03:00,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8778, 'grad_norm': 5.829656600952148, 'learning_rate': 2.058823529411765e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 67%|██████▋   | 68/102 [05:27<02:26,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.278550386428833, 'eval_runtime': 3.7195, 'eval_samples_per_second': 2.151, 'eval_steps_per_second': 1.075, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 70/102 [05:39<03:06,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9584, 'grad_norm': 4.02698278427124, 'learning_rate': 1.568627450980392e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 80/102 [06:22<01:37,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8315, 'grad_norm': 4.547756671905518, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 90/102 [07:07<00:52,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7833, 'grad_norm': 5.019215106964111, 'learning_rate': 5.882352941176471e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 100/102 [07:49<00:08,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8395, 'grad_norm': 4.409575462341309, 'learning_rate': 9.80392156862745e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 102/102 [08:04<00:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.304596185684204, 'eval_runtime': 3.886, 'eval_samples_per_second': 2.059, 'eval_steps_per_second': 1.029, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [08:07<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 487.7477, 'train_samples_per_second': 0.418, 'train_steps_per_second': 0.209, 'train_loss': 0.9467497853671804, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=0.9467497853671804, metrics={'train_runtime': 487.7477, 'train_samples_per_second': 0.418, 'train_steps_per_second': 0.209, 'total_flos': 15948419235840.0, 'train_loss': 0.9467497853671804, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================\n",
    "# 8) Fine-Tune\n",
    "# ======================================\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BART model + tokenizer saved in: bart_partial_entire_model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 9) Save Fine-Tuned Model\n",
    "# ======================================\n",
    "save_dir = \"bart_partial_entire_model\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Fine-tuned BART model + tokenizer saved in:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
