{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript to fine-tune BART for conversation prediction (partial -> remainder).\\n\\nDifferences from previous approach:\\n- Uses a simpler Hugging Face Dataset creation approach with map() for tokenization\\n- Minimal usage of DataCollatorForSeq2Seq\\n- Same logic of partial->remainder structure\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to fine-tune BART for conversation prediction (partial -> remainder).\n",
    "\n",
    "Differences from previous approach:\n",
    "- Uses a simpler Hugging Face Dataset creation approach with map() for tokenization\n",
    "- Minimal usage of DataCollatorForSeq2Seq\n",
    "- Same logic of partial->remainder structure\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1) Imports\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2) CSV -> Partial->Remainder\n",
    "# ==========================================\n",
    "def build_partial_remainder(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Read CSV with columns like:\n",
    "      CONVERSATION_ID, CONVERSATION_STEP, TEXT\n",
    "    Group lines by conversation, sort by step, \n",
    "    build partial (first X% lines) vs remainder (last X% lines).\n",
    "    Return list of (partial_str, remainder_str).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "        cutoff = max(1, int(len(texts)*partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list= texts[cutoff:]\n",
    "        partial_str = \"\\n\".join(partial_list).strip()\n",
    "        remainder_str= \"\\n\".join(remainder_list).strip()\n",
    "        if partial_str and remainder_str:\n",
    "            pairs.append((partial_str, remainder_str))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Sample pair:\n",
      "Partial: Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? \n",
      "Remainder: You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee?\n",
      "Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com.\n",
      "Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help!\n",
      "You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  \n",
    "partial_ratio = 0.5\n",
    "pairs = build_partial_remainder(csv_path, partial_ratio)\n",
    "print(f\"Number of partial->remainder pairs: {len(pairs)}\")\n",
    "if pairs:\n",
    "    print(\"Sample pair:\\nPartial:\", pairs[0][0], \"\\nRemainder:\", pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 76\n",
      "})\n",
      "Sample record: {'source': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'target': \"You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3) Create a HuggingFace Datasets object\n",
    "#    from the partial->remainder pairs\n",
    "# ==========================================\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def create_hf_dataset(pairs):\n",
    "    # We'll build a list of dicts: { \"source\":..., \"target\":... }\n",
    "    data_dict = {\n",
    "        \"source\": [p[0] for p in pairs],\n",
    "        \"target\": [p[1] for p in pairs]\n",
    "    }\n",
    "    hf_ds = HFDataset.from_dict(data_dict)\n",
    "    return hf_ds\n",
    "\n",
    "hf_ds = create_hf_dataset(pairs)\n",
    "print(hf_ds)\n",
    "if len(hf_ds)>0:\n",
    "    print(\"Sample record:\", hf_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF train size: 68\n",
      "HF val size  : 8\n"
     ]
    }
   ],
   "source": [
    "# We'll do a train/val split\n",
    "train_size = int(0.9 * len(hf_ds))\n",
    "val_size   = len(hf_ds) - train_size\n",
    "hf_train, hf_val = hf_ds.train_test_split(test_size=val_size).values()\n",
    "\n",
    "print(\"HF train size:\", len(hf_train))\n",
    "print(\"HF val size  :\", len(hf_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BART + Tokenizer from: facebook/bart-base\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Load BART + Tokenizer\n",
    "# ==========================================\n",
    "model_name = \"facebook/bart-base\"  # or bart-large, etc.\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "print(\"Loaded BART + Tokenizer from:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/68 [00:00<?, ? examples/s]c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 71.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 157.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train sample: {'input_ids': tensor([    0,   713,  2175,    16,   223,   110,   766,     6,    45,   223,\n",
      "           84,   827,    18,   766,     4, 50118,  9904,     6,  4420,     4,\n",
      "        50118, 32730,     6,    52,  1395,  1719,     5,   455,   379,   153,\n",
      "        20858,   771,    19,    84,   827,    18,  1188,     4, 50118,  9904,\n",
      "            6,  4420,     4, 50118, 13984,     6,   965,    75,    14,  4577,\n",
      "          116, 50118,  9904,     6,  4420,     4, 50118,  1106,    84,   827,\n",
      "        14617,    24,  4378,     6,    24,   115,    28, 32085,    25,    10,\n",
      "        15178,  4628,  2541,     4, 50118,  9904,     4, 50118,   170,    40,\n",
      "         1719,   843,    12,  1096,   207,    31,     5,  1049,  6084,     8,\n",
      "          291,    12,   541,   207,    31,     5,   400,  6084,     4, 50118,\n",
      "         9904,     4, 50118,   133,  2405,   291,    12,   541,   207,   782,\n",
      "            7,    28,  2913,    30,    47,     6,     8,    52,   581, 27736,\n",
      "           47,   423,     4, 50118,  1711,  4428,  1202,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([    0,   243,    17,    27,    29,    45,    59,     5,  1280,     6,\n",
      "           38,    95,   218,    17,    27,    90,   236,     7,   185, 10495,\n",
      "         2476,     4, 50118,  1708,    42,    16, 10142,    13,    47,     4,\n",
      "        50118,   100,  1346,     6,    53,     5,  1068,    16, 17322,    13,\n",
      "          162,     4, 50118,  1106,    47,  2845,    45,     7,  9073,     6,\n",
      "           24,   189,   483,     7,  2207,     8, 12385,     4, 50118,   100,\n",
      "         1346,    14,     6,    53,     5,   613,  6976,    16,   350,  2016,\n",
      "            4, 50118,  2264,   114,    38,  1719,  3982,     6,   151, 20858,\n",
      "          771,    31,   127,  1188,     7,   244,    47,   116, 50118,  1779,\n",
      "           74,     5, 22507,  1369,   114,    38,  9073,   116, 50118, 35808,\n",
      "          624,   155,    12,   306,   722,    71,  5774,     5,  2339,     4,\n",
      "        50118,   100,    17,    27,   119,    45,   584,    38, 27948,    47,\n",
      "            6,    53,    42,   609,  2653, 11788,     4,     2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5) Tokenization Function\n",
    "# ==========================================\n",
    "def tokenize_function(examples):\n",
    "    # For BART seq2seq: \n",
    "    # \"source\" -> encoder input\n",
    "    # \"target\" -> decoder labels\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"], \n",
    "        max_length=128,    # <--- param: tune\n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"], \n",
    "            max_length=128,  # <--- param: tune\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "hf_train = hf_train.map(tokenize_function, batched=True, remove_columns=[\"source\",\"target\"])\n",
    "hf_val   = hf_val.map(tokenize_function,   batched=True, remove_columns=[\"source\",\"target\"])\n",
    "\n",
    "hf_train.set_format(\"torch\")\n",
    "hf_val.set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized train sample:\", hf_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6) DataCollatorForSeq2Seq\n",
    "# ==========================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15632\\3276508228.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7) Trainer Setup\n",
    "# ==========================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bart_conversation_predict_2\",  # <--- param: where to save\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                       # <--- param: tune\n",
    "    per_device_train_batch_size=2,            # <--- param: tune\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,                       # <--- param: tune\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\data\\data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      " 10%|â–‰         | 10/102 [00:48<05:13,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.887, 'grad_norm': 9.756758689880371, 'learning_rate': 4.5098039215686275e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 20/102 [01:20<04:18,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3955, 'grad_norm': 10.599475860595703, 'learning_rate': 4.0196078431372555e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 30/102 [01:52<03:48,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1173, 'grad_norm': 8.892078399658203, 'learning_rate': 3.529411764705883e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 34/102 [02:09<03:56,  3.48s/it]c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6425771713256836, 'eval_runtime': 2.3227, 'eval_samples_per_second': 3.444, 'eval_steps_per_second': 1.722, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 40/102 [02:32<03:40,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7964, 'grad_norm': 8.188032150268555, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 50/102 [03:03<02:44,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5413, 'grad_norm': 8.344756126403809, 'learning_rate': 2.5490196078431373e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 60/102 [03:36<02:16,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6826, 'grad_norm': 8.38615608215332, 'learning_rate': 2.058823529411765e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 68/102 [04:05<01:48,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4609251022338867, 'eval_runtime': 2.2121, 'eval_samples_per_second': 3.616, 'eval_steps_per_second': 1.808, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 70/102 [04:14<02:21,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.436, 'grad_norm': 6.876885414123535, 'learning_rate': 1.568627450980392e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 80/102 [04:46<01:10,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2743, 'grad_norm': 8.150529861450195, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 90/102 [05:17<00:37,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2932, 'grad_norm': 8.273405075073242, 'learning_rate': 5.882352941176471e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 100/102 [05:48<00:06,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2278, 'grad_norm': 8.27377700805664, 'learning_rate': 9.80392156862745e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [06:00<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4451236724853516, 'eval_runtime': 2.3735, 'eval_samples_per_second': 3.371, 'eval_steps_per_second': 1.685, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [06:03<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 363.3148, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.281, 'train_loss': 2.756316970376407, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=2.756316970376407, metrics={'train_runtime': 363.3148, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.281, 'total_flos': 14915916288000.0, 'train_loss': 2.756316970376407, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8) Fine-Tune\n",
    "# ==========================================\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BART model + tokenizer saved to: bart_conversation_finetuned_model_2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9) Save Fine-Tuned Model\n",
    "# ==========================================\n",
    "save_dir = \"bart_conversation_finetuned_model_2\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Fine-tuned BART model + tokenizer saved to:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
