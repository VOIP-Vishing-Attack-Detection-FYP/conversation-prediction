{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Prediction with a Custom Encoder-Decoder,\\nusing GPT-2 Tokenizer for subword merges.\\n\\nSteps:\\n1) Load a CSV of conversation data, group by CONVERSATION_ID, and build\\n   (partial, remainder) pairs.\\n2) Use GPT2Tokenizer from transformers for tokenization (subword approach).\\n3) Build a minimal from-scratch encoder-decoder Transformer in PyTorch.\\n4) Train the model on partial->remainder generation.\\n5) Demonstrate inference (greedy decoding).\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conversation Prediction with a Custom Encoder-Decoder,\n",
    "using GPT-2 Tokenizer for subword merges.\n",
    "\n",
    "Steps:\n",
    "1) Load a CSV of conversation data, group by CONVERSATION_ID, and build\n",
    "   (partial, remainder) pairs.\n",
    "2) Use GPT2Tokenizer from transformers for tokenization (subword approach).\n",
    "3) Build a minimal from-scratch encoder-decoder Transformer in PyTorch.\n",
    "4) Train the model on partial->remainder generation.\n",
    "5) Demonstrate inference (greedy decoding).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1) Imports\n",
    "# ========================================\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Sample pair:\n",
      " {'partial': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'remainder': \"You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 2) CSV -> Partial->Remainder\n",
    "# ========================================\n",
    "def build_convo_pairs(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Reads CSV with columns:\n",
    "       CONVERSATION_ID, CONVERSATION_STEP, TEXT\n",
    "    Groups by conversation ID, sorts by step, then\n",
    "    splits into (partial, remainder) text segments.\n",
    "\n",
    "    partial_ratio determines the fraction of lines\n",
    "    used for partial snippet, remainder is the rest.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data_pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "\n",
    "        cutoff = max(1, int(len(texts)*partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list = texts[cutoff:]\n",
    "\n",
    "        partial_str = \"\\n\".join(partial_list).strip()\n",
    "        remainder_str = \"\\n\".join(remainder_list).strip() if remainder_list else \"\"\n",
    "\n",
    "        data_pairs.append({\n",
    "            \"partial\": partial_str,\n",
    "            \"remainder\": remainder_str\n",
    "        })\n",
    "\n",
    "    return data_pairs\n",
    "\n",
    "# Example usage (Update csv_path to your actual path)\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  # <-- update with your CSV path\n",
    "conversation_data = build_convo_pairs(csv_path, partial_ratio=0.5)\n",
    "print(\"Number of partial->remainder pairs:\", len(conversation_data))\n",
    "if len(conversation_data) > 0:\n",
    "    print(\"Sample pair:\\n\", conversation_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Vocab size: 50257\n",
      "EOS token id: 50256\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3) GPT-2 Tokenizer Setup\n",
    "# ========================================\n",
    "# We'll use GPT2Tokenizer for subword merges.\n",
    "# GPT-2 doesn't define a pad token by default, so we\n",
    "# set pad_token = eos_token for convenience.\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # reuse <|endoftext|> as pad\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"GPT2 Vocab size:\", len(tokenizer))\n",
    "print(\"EOS token id:\", pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_batch shape: torch.Size([2, 64])\n",
      "remainder_batch shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4) PyTorch Dataset\n",
    "# ========================================\n",
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: partial -> remainder text.\n",
    "    We'll tokenize with GPT2Tokenizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_pairs, tokenizer, max_len=64):\n",
    "        self.data = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        partial_text = ex[\"partial\"]\n",
    "        remainder_text= ex[\"remainder\"]\n",
    "\n",
    "        enc_partial = self.tokenizer.encode(\n",
    "            partial_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True\n",
    "        )\n",
    "        enc_remainder = self.tokenizer.encode(\n",
    "            remainder_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"enc_partial\": enc_partial,\n",
    "            \"enc_remainder\": enc_remainder\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    partial_list = [b[\"enc_partial\"] for b in batch]\n",
    "    remainder_list= [b[\"enc_remainder\"] for b in batch]\n",
    "\n",
    "    max_len_enc = max(len(p) for p in partial_list)\n",
    "    max_len_dec = max(len(r) for r in remainder_list)\n",
    "\n",
    "    partial_padded = []\n",
    "    remainder_padded= []\n",
    "\n",
    "    for p, r in zip(partial_list, remainder_list):\n",
    "        p_pad = p + [pad_token_id]*(max_len_enc - len(p))\n",
    "        r_pad = r + [pad_token_id]*(max_len_dec - len(r))\n",
    "        partial_padded.append(p_pad)\n",
    "        remainder_padded.append(r_pad)\n",
    "\n",
    "    return {\n",
    "        \"partial_batch\": torch.tensor(partial_padded, dtype=torch.long),\n",
    "        \"remainder_batch\": torch.tensor(remainder_padded, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "dataset = ConversationDataset(conversation_data, tokenizer, max_len=64)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# quick test\n",
    "for batch in loader:\n",
    "    print(\"partial_batch shape:\", batch[\"partial_batch\"].shape)\n",
    "    print(\"remainder_batch shape:\", batch[\"remainder_batch\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5) Minimal Encoder-Decoder from scratch\n",
    "# ========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :].to(x.device)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, pad_token_id=50256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def _generate_causal_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz))==1).transpose(0,1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src, tgt shapes: [B, seq_len]\n",
    "        enc_emb = self.embedding(src)*math.sqrt(self.d_model)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "\n",
    "        dec_emb = self.embedding(tgt)*math.sqrt(self.d_model)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "\n",
    "        src_key_padding_mask = (src==self.pad_token_id)\n",
    "        tgt_key_padding_mask = (tgt==self.pad_token_id)\n",
    "\n",
    "        seq_len_dec = tgt.size(1)\n",
    "        causal_mask = self._generate_causal_mask(seq_len_dec).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=enc_emb,\n",
    "            tgt=dec_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        logits = self.fc_out(out)  # [B, tgt_len, vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss=8.4660\n",
      "Epoch 2/5, Loss=5.7548\n",
      "Epoch 3/5, Loss=4.9522\n",
      "Epoch 4/5, Loss=4.3168\n",
      "Epoch 5/5, Loss=3.7345\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6) Train\n",
    "# ========================================\n",
    "vocab_size = len(tokenizer)\n",
    "model = TransformerEncoderDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for batch in loader:\n",
    "        partial_batch = batch[\"partial_batch\"].to(device)\n",
    "        remainder_batch= batch[\"remainder_batch\"].to(device)\n",
    "\n",
    "        # teacher forcing\n",
    "        dec_in = remainder_batch[:, :-1]\n",
    "        labels = remainder_batch[:, 1:].contiguous()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(partial_batch, dec_in)  # [B, dec_len, vocab_size]\n",
    "\n",
    "        B,L,V = logits.shape\n",
    "        loss = criterion(logits.view(B*L,V), labels.view(B*L))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss={avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTIAL ===\n",
      "Hello, I'm Sam. I saw an ad about a photography workshop.\n",
      "\n",
      "=== PREDICTED REMAINDER ===\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7) Inference: Generating the Remainder\n",
    "# ========================================\n",
    "def generate_remainder(model, tokenizer, partial_text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Provide partial text -> model tries to produce remainder.\n",
    "    We do a simple greedy decode step-by-step\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # encode partial\n",
    "        enc_partial = tokenizer.encode(partial_text, add_special_tokens=False)\n",
    "        src = torch.tensor([enc_partial], dtype=torch.long).to(device)\n",
    "\n",
    "        # start decoder with e.g. [pad/eos], or any dummy \"start\" token\n",
    "        dec_in = [tokenizer.eos_token_id]  # placeholder as \"BOS\"\n",
    "        dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(src, dec_tensor)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "            dec_in.append(next_id)\n",
    "            dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "            if next_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # skip the first token (the placeholder)\n",
    "        generated_ids = dec_in[1:]\n",
    "        text_out = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return text_out\n",
    "\n",
    "# test on a partial snippet\n",
    "test_partial = \"Hello, I'm Sam. I saw an ad about a photography workshop.\"\n",
    "gen_remainder = generate_remainder(model, tokenizer, test_partial, max_new_tokens=30)\n",
    "print(\"=== PARTIAL ===\")\n",
    "print(test_partial)\n",
    "print(\"\\n=== PREDICTED REMAINDER ===\")\n",
    "print(gen_remainder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
