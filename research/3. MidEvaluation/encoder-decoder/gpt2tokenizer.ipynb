{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Prediction Notebook\\n- Takes a CSV file with columns:\\n    CONVERSATION_ID, CONVERSATION_STEP, TEXT, ...\\n- Splits each conversation into partial->remainder\\n- Trains a from-scratch encoder-decoder Transformer to generate the remainder\\n  given only the partial snippet.\\n- Saves the model + tokenizer for reuse.\\n- Demonstrates how to load and run inference on the saved model.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conversation Prediction Notebook\n",
    "- Takes a CSV file with columns:\n",
    "    CONVERSATION_ID, CONVERSATION_STEP, TEXT, ...\n",
    "- Splits each conversation into partial->remainder\n",
    "- Trains a from-scratch encoder-decoder Transformer to generate the remainder\n",
    "  given only the partial snippet.\n",
    "- Saves the model + tokenizer for reuse.\n",
    "- Demonstrates how to load and run inference on the saved model.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1) Imports\n",
    "# ========================================\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle  # for saving tokenizer or other objects\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2) CSV -> Partial->Remainder\n",
    "# ========================================\n",
    "def build_convo_pairs(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Reads the CSV with columns: \n",
    "      CONVERSATION_ID, CONVERSATION_STEP, TEXT\n",
    "    Groups by conversation ID, sorts by step,\n",
    "    then splits into (partial, remainder).\n",
    "    \n",
    "    partial_ratio indicates what fraction of lines \n",
    "    in each conversation is used as 'partial' snippet.\n",
    "    The remainder lines form the 'remainder' text.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data_pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "\n",
    "        cutoff = max(1, int(len(texts)*partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list = texts[cutoff:]\n",
    "\n",
    "        partial_str = \"\\n\".join(partial_list).strip()\n",
    "        remainder_str = \"\\n\".join(remainder_list).strip() if remainder_list else \"\"\n",
    "\n",
    "        data_pairs.append({\n",
    "            \"partial\": partial_str,\n",
    "            \"remainder\": remainder_str\n",
    "        })\n",
    "\n",
    "    return data_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Sample pair:\n",
      " {'partial': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'remainder': \"You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# Example usage (update csv_path for your real data)\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  # <-- update with your CSV path\n",
    "partial_ratio = 0.5\n",
    "\n",
    "conversation_data = build_convo_pairs(csv_path, partial_ratio)\n",
    "print(\"Number of partial->remainder pairs:\", len(conversation_data))\n",
    "if len(conversation_data) > 0:\n",
    "    print(\"Sample pair:\\n\", conversation_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3) A Minimal Tokenizer (Word-Based)\n",
    "# ========================================\n",
    "# For demonstration, we build a naive word-level tokenizer that:\n",
    "#  - uses <PAD> (0), <BOS> (1), <EOS> (2), <UNK> (3)\n",
    "#  - collects all unique tokens from partial + remainder text\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\":0, \"<BOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.idx2word = {0:\"<PAD>\",1:\"<BOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        self.vocab_size = 4\n",
    "\n",
    "    def fit(self, data_pairs):\n",
    "        idx = 4\n",
    "        for ex in data_pairs:\n",
    "            combined = ex[\"partial\"] + \" \" + ex[\"remainder\"]\n",
    "            words = combined.split()\n",
    "            for w in words:\n",
    "                if w not in self.word2idx:\n",
    "                    self.word2idx[w] = idx\n",
    "                    self.idx2word[idx] = w\n",
    "                    idx += 1\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, text, add_bos=False, add_eos=False):\n",
    "        ids = []\n",
    "        if add_bos:\n",
    "            ids.append(self.word2idx[\"<BOS>\"])\n",
    "        for w in text.split():\n",
    "            wid = self.word2idx.get(w, self.word2idx[\"<UNK>\"])\n",
    "            ids.append(wid)\n",
    "        if add_eos:\n",
    "            ids.append(self.word2idx[\"<EOS>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            if i==self.word2idx[\"<EOS>\"]:\n",
    "                break\n",
    "            word = self.idx2word.get(i, \"<UNK>\")\n",
    "            words.append(word)\n",
    "        return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2942\n"
     ]
    }
   ],
   "source": [
    "# Build the tokenizer from conversation_data\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(conversation_data)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "# Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4) PyTorch Dataset\n",
    "# ========================================\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, data_pairs, tokenizer, max_len=50):\n",
    "        self.data = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        partial_enc = self.tokenizer.encode(ex[\"partial\"], add_bos=True, add_eos=True)\n",
    "        remainder_enc = self.tokenizer.encode(ex[\"remainder\"], add_bos=True, add_eos=True)\n",
    "\n",
    "        # Truncate if too long\n",
    "        partial_enc = partial_enc[:self.max_len]\n",
    "        remainder_enc = remainder_enc[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            \"partial_enc\": partial_enc,\n",
    "            \"remainder_enc\": remainder_enc\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    partial_list = [b[\"partial_enc\"] for b in batch]\n",
    "    remainder_list = [b[\"remainder_enc\"] for b in batch]\n",
    "\n",
    "    max_len_enc = max(len(p) for p in partial_list)\n",
    "    max_len_dec = max(len(r) for r in remainder_list)\n",
    "\n",
    "    partial_padded = []\n",
    "    remainder_padded= []\n",
    "\n",
    "    for p, r in zip(partial_list, remainder_list):\n",
    "        p_pad = p + [tokenizer.word2idx[\"<PAD>\"]] * (max_len_enc-len(p))\n",
    "        r_pad = r + [tokenizer.word2idx[\"<PAD>\"]] * (max_len_dec-len(r))\n",
    "        partial_padded.append(p_pad)\n",
    "        remainder_padded.append(r_pad)\n",
    "\n",
    "    return {\n",
    "        \"partial_batch\": torch.tensor(partial_padded, dtype=torch.long),\n",
    "        \"remainder_batch\": torch.tensor(remainder_padded, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "dataset = ConversationDataset(conversation_data, tokenizer, max_len=50)\n",
    "loader  = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_batch shape: torch.Size([2, 50])\n",
      "remainder_batch shape: torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "for batch in loader:\n",
    "    print(\"partial_batch shape:\", batch[\"partial_batch\"].shape)\n",
    "    print(\"remainder_batch shape:\", batch[\"remainder_batch\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5) A Minimal Encoder-Decoder Model\n",
    "# ========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :].to(x.device)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.pad_token_id = 0  # <PAD> index\n",
    "\n",
    "    def _generate_causal_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size))==1).transpose(0,1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: [B, src_len]\n",
    "        # tgt: [B, tgt_len]\n",
    "        # embeddings\n",
    "        enc_emb = self.embedding(src)*math.sqrt(self.d_model)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "\n",
    "        dec_emb = self.embedding(tgt)*math.sqrt(self.d_model)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "\n",
    "        # create masks\n",
    "        src_key_padding_mask = (src==self.pad_token_id)\n",
    "        tgt_key_padding_mask = (tgt==self.pad_token_id)\n",
    "\n",
    "        seq_len_tgt = tgt.size(1)\n",
    "        causal_mask = self._generate_causal_mask(seq_len_tgt).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=enc_emb, \n",
    "            tgt=dec_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        logits = self.fc_out(out)  # [B, tgt_len, vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss=7.0616\n",
      "Epoch 2/5, Loss=6.2368\n",
      "Epoch 3/5, Loss=6.0195\n",
      "Epoch 4/5, Loss=5.4625\n",
      "Epoch 5/5, Loss=4.7270\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6) Train the Model\n",
    "# ========================================\n",
    "model = TransformerEncoderDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx[\"<PAD>\"])\n",
    "\n",
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for batch in loader:\n",
    "        partial_batch = batch[\"partial_batch\"].to(device)\n",
    "        remainder_batch = batch[\"remainder_batch\"].to(device)\n",
    "\n",
    "        # teacher forcing\n",
    "        # dec_in => remainder[:,:-1]\n",
    "        # label => remainder[:,1:]\n",
    "        dec_in = remainder_batch[:, :-1]\n",
    "        labels = remainder_batch[:, 1:].contiguous()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(partial_batch, dec_in)  # [B, dec_len, vocab_size]\n",
    "        B,L,V = logits.shape\n",
    "        loss = criterion(logits.view(B*L,V), labels.view(B*L))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: gpt2_saved_conversation_model\\model_state.pt\n",
      "Tokenizer saved to: gpt2_saved_conversation_model\\tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7) Save the Model + Tokenizer\n",
    "# ========================================\n",
    "save_dir = \"gpt2_saved_conversation_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model state_dict\n",
    "model_path = os.path.join(save_dir, \"model_state.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_path = os.path.join(save_dir, \"tokenizer.pkl\")\n",
    "with open(tokenizer_path, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(f\"Tokenizer saved to: {tokenizer_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 8) Reusable Inference Code\n",
    "# ========================================\n",
    "# Suppose we want to load the model + tokenizer in a new environment or script \n",
    "# and generate the remainder from a partial snippet.\n",
    "\n",
    "def load_model_and_tokenizer(model_dir):\n",
    "    # Load tokenizer\n",
    "    tokenizer_path = os.path.join(model_dir, \"tokenizer.pkl\")\n",
    "    with open(tokenizer_path, \"rb\") as f:\n",
    "        loaded_tokenizer = pickle.load(f)\n",
    "    # Instantiate model with correct vocab size\n",
    "    # Make sure you match d_model, nhead, layers, etc. \n",
    "    # or store them in a config file\n",
    "    model = TransformerEncoderDecoder(\n",
    "        vocab_size=loaded_tokenizer.vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2\n",
    "    )\n",
    "    # Load state_dict\n",
    "    model_path = os.path.join(model_dir, \"model_state.pt\")\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, loaded_tokenizer\n",
    "\n",
    "def generate_remainder(model, tokenizer, partial_text, max_len=50):\n",
    "    model.eval()\n",
    "    # encode partial\n",
    "    partial_ids = tokenizer.encode(partial_text, add_bos=True)  # add <BOS>\n",
    "    src = torch.tensor([partial_ids], dtype=torch.long)\n",
    "\n",
    "    # We start decoder input with <BOS>\n",
    "    dec_in = torch.tensor([[tokenizer.word2idx[\"<BOS>\"]]], dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(src, dec_in)\n",
    "            # next token from last time-step\n",
    "            next_token_logits = logits[0,-1,:]\n",
    "            next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "        # append\n",
    "        dec_in = torch.cat([dec_in, torch.tensor([[next_id]])], dim=1)\n",
    "\n",
    "        # stop if <EOS>\n",
    "        if next_id == tokenizer.word2idx[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    # decode the generated tokens after <BOS>\n",
    "    generated_ids = dec_in[0,1:].tolist()\n",
    "    text_out = tokenizer.decode(generated_ids)\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuse the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23988\\3588200497.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTIAL ===\n",
      "Hello, I'm Sam. I saw an ad about a photography workshop.\n",
      "\n",
      "=== PREDICTED REMAINDER ===\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Example of usage:\n",
    "\n",
    "# (A) In the same script, we can simply reuse the 'model' and 'tokenizer' we already have.\n",
    "# Or let's simulate how we'd do it from a fresh environment:\n",
    "\n",
    "loaded_model, loaded_tokenizer = load_model_and_tokenizer(\"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/research/3. MidEvaluation/encoder-decoder/gpt2_saved_conversation_model\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Now let's pick a partial snippet from the dataset or custom:\n",
    "test_partial = \"Hello, I'm Sam. I saw an ad about a photography workshop.\"\n",
    "print(\"=== PARTIAL ===\")\n",
    "print(test_partial)\n",
    "\n",
    "remainder_pred = generate_remainder(loaded_model, loaded_tokenizer, test_partial)\n",
    "print(\"\\n=== PREDICTED REMAINDER ===\")\n",
    "print(remainder_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
