{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Prediction (Partial->Remainder) with a From-Scratch Encoder-Decoder\\nusing BERT Tokenizer for subword merges.\\n\\nSteps:\\n1) Read CSV of conversation data, build partial->remainder pairs.\\n2) Tokenize with BertTokenizer (WordPiece).\\n3) Train a minimal Transformer-based seq2seq.\\n4) Save model + tokenizer for reuse.\\n5) Load model + tokenizer and run inference to generate remainder from partial snippet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conversation Prediction (Partial->Remainder) with a From-Scratch Encoder-Decoder\n",
    "using BERT Tokenizer for subword merges.\n",
    "\n",
    "Steps:\n",
    "1) Read CSV of conversation data, build partial->remainder pairs.\n",
    "2) Tokenize with BertTokenizer (WordPiece).\n",
    "3) Train a minimal Transformer-based seq2seq.\n",
    "4) Save model + tokenizer for reuse.\n",
    "5) Load model + tokenizer and run inference to generate remainder from partial snippet.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1) Imports\n",
    "# ========================================\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Sample pair:\n",
      " {'partial': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'remainder': \"You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 2) CSV -> Partial->Remainder\n",
    "# ========================================\n",
    "def build_convo_pairs(csv_path, partial_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Reads CSV with columns (CONVERSATION_ID, CONVERSATION_STEP, TEXT, ...)\n",
    "    Groups lines by conversation, sorts by step, then splits each conversation\n",
    "    into partial vs remainder text segments based on partial_ratio.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data_pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "\n",
    "        cutoff = max(1, int(len(texts)*partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list = texts[cutoff:]\n",
    "\n",
    "        partial_str = \"\\n\".join(partial_list).strip()\n",
    "        remainder_str = \"\\n\".join(remainder_list).strip() if remainder_list else \"\"\n",
    "\n",
    "        data_pairs.append({\n",
    "            \"partial\": partial_str,\n",
    "            \"remainder\": remainder_str\n",
    "        })\n",
    "    return data_pairs\n",
    "\n",
    "# Example usage (adapt the path):\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  # <-- update with your CSV path\n",
    "data_pairs = build_convo_pairs(csv_path, partial_ratio=0.5)\n",
    "print(\"Number of partial->remainder pairs:\", len(data_pairs))\n",
    "if len(data_pairs)>0:\n",
    "    print(\"Sample pair:\\n\", data_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n",
      "Pad token: [PAD] ID: 0\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3) BERT Tokenizer Setup\n",
    "# ========================================\n",
    "# We'll use the \"bert-base-uncased\" tokenizer from Hugging Face.\n",
    "# BERT typically has [PAD], [CLS], [SEP], etc. We'll rely on [PAD] for padding.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "print(\"Vocab size:\", len(tokenizer))\n",
    "print(\"Pad token:\", tokenizer.pad_token, \"ID:\", pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_batch shape: torch.Size([2, 64])\n",
      "remainder_batch shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4) Dataset + Collate\n",
    "# ========================================\n",
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: partial -> remainder text\n",
    "    We'll tokenize with the BERT tokenizer (WordPiece).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_pairs, tokenizer, max_len=64):\n",
    "        self.data = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        partial_text = ex[\"partial\"]\n",
    "        remainder_text= ex[\"remainder\"]\n",
    "\n",
    "        # encode partial\n",
    "        enc_partial = self.tokenizer.encode(\n",
    "            partial_text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        # encode remainder\n",
    "        enc_remainder= self.tokenizer.encode(\n",
    "            remainder_text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"enc_partial\": enc_partial,\n",
    "            \"enc_remainder\": enc_remainder\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    partial_list = [b[\"enc_partial\"] for b in batch]\n",
    "    remainder_list= [b[\"enc_remainder\"] for b in batch]\n",
    "\n",
    "    max_len_enc = max(len(p) for p in partial_list)\n",
    "    max_len_dec = max(len(r) for r in remainder_list)\n",
    "\n",
    "    partial_padded = []\n",
    "    remainder_padded= []\n",
    "\n",
    "    for p, r in zip(partial_list, remainder_list):\n",
    "        p_pad = p + [pad_token_id]*(max_len_enc-len(p))\n",
    "        r_pad = r + [pad_token_id]*(max_len_dec-len(r))\n",
    "        partial_padded.append(p_pad)\n",
    "        remainder_padded.append(r_pad)\n",
    "\n",
    "    return {\n",
    "        \"partial_batch\": torch.tensor(partial_padded, dtype=torch.long),\n",
    "        \"remainder_batch\": torch.tensor(remainder_padded, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "dataset = ConversationDataset(data_pairs, tokenizer, max_len=64)\n",
    "loader  = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# quick test\n",
    "for batch in loader:\n",
    "    print(\"partial_batch shape:\", batch[\"partial_batch\"].shape)\n",
    "    print(\"remainder_batch shape:\", batch[\"remainder_batch\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5) Minimal Encoder-Decoder\n",
    "# ========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        pad_token_id=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out_fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def _generate_causal_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz))==1\n",
    "        mask = mask.transpose(0,1).masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src, tgt shapes: [B, seq_len]\n",
    "        \"\"\"\n",
    "        enc_emb = self.embedding(src)*math.sqrt(self.d_model)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "\n",
    "        dec_emb = self.embedding(tgt)*math.sqrt(self.d_model)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "\n",
    "        src_key_padding_mask = (src==self.pad_token_id)\n",
    "        tgt_key_padding_mask = (tgt==self.pad_token_id)\n",
    "\n",
    "        seq_len_dec = tgt.size(1)\n",
    "        causal_mask = self._generate_causal_mask(seq_len_dec).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=enc_emb,\n",
    "            tgt=dec_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        logits = self.out_fc(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss=7.8924\n",
      "Epoch 2/10, Loss=5.3842\n",
      "Epoch 3/10, Loss=4.6943\n",
      "Epoch 4/10, Loss=4.1671\n",
      "Epoch 5/10, Loss=3.7374\n",
      "Epoch 6/10, Loss=3.3957\n",
      "Epoch 7/10, Loss=3.1258\n",
      "Epoch 8/10, Loss=2.9069\n",
      "Epoch 9/10, Loss=2.7317\n",
      "Epoch 10/10, Loss=2.6361\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6) Training\n",
    "# ========================================\n",
    "vocab_size = len(tokenizer)\n",
    "model = TransformerEncoderDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for batch in loader:\n",
    "        partial_batch = batch[\"partial_batch\"].to(device)\n",
    "        remainder_batch= batch[\"remainder_batch\"].to(device)\n",
    "\n",
    "        # teacher forcing\n",
    "        dec_in = remainder_batch[:, :-1]\n",
    "        labels = remainder_batch[:, 1:].contiguous()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(partial_batch, dec_in)  # [B, dec_len, vocab_size]\n",
    "        B,L,V = logits.shape\n",
    "        loss = criterion(logits.view(B*L,V), labels.view(B*L))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to conversation_model_bert_tokenizer\\model_state.pt\n",
      "Tokenizer saved to conversation_model_bert_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7) Save Model + Tokenizer\n",
    "# ========================================\n",
    "save_dir = \"conversation_model_bert_tokenizer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(save_dir, \"model_state.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# We can save the tokenizer in Hugging Face format\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Tokenizer saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 8) Inference: Generating the Remainder\n",
    "# ========================================\n",
    "def load_model_and_tokenizer(model_dir):\n",
    "    from transformers import BertTokenizer\n",
    "    # Load tokenizer\n",
    "    loaded_tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # We must reconstruct the same model architecture\n",
    "    # (ideally, store hyperparams in a config file)\n",
    "    model = TransformerEncoderDecoder(\n",
    "        vocab_size=len(loaded_tokenizer),\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        pad_token_id=loaded_tokenizer.pad_token_id\n",
    "    )\n",
    "    # Load weights\n",
    "    model_state_path = os.path.join(model_dir, \"model_state.pt\")\n",
    "    model.load_state_dict(torch.load(model_state_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, loaded_tokenizer\n",
    "\n",
    "def generate_remainder(model, tokenizer, partial_text, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_partial = tokenizer.encode(\n",
    "            partial_text,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        src = torch.tensor([enc_partial], dtype=torch.long).to(device)\n",
    "\n",
    "        # We'll start the decoder with a single pad token to serve as \"BOS\"\n",
    "        dec_in = [tokenizer.pad_token_id]\n",
    "        dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(src, dec_tensor)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "            dec_in.append(next_id)\n",
    "            dec_tensor = torch.tensor([dec_in], dtype=torch.long).to(device)\n",
    "\n",
    "            # Stop if we produce a pad token again or if you define some other stopping\n",
    "            if next_id == tokenizer.pad_token_id:\n",
    "                break\n",
    "\n",
    "        # decode the subwords after the initial \"pad\" start\n",
    "        generated_ids = dec_in[1:]\n",
    "        text_out = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTIAL ===\n",
      "Hello, I'm Sam. I saw an ad about a photography workshop.\n",
      "\n",
      "=== PREDICTED REMAINDER ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6668\\1533247914.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_state_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "loaded_model, loaded_tok = load_model_and_tokenizer(save_dir)\n",
    "\n",
    "test_partial = \"Hello, I'm Sam. I saw an ad about a photography workshop.\"\n",
    "print(\"=== PARTIAL ===\")\n",
    "print(test_partial)\n",
    "res = generate_remainder(loaded_model, loaded_tok, test_partial)\n",
    "print(\"\\n=== PREDICTED REMAINDER ===\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
