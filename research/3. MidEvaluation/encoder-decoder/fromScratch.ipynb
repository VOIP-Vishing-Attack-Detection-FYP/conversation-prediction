{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Prediction from CSV in a Jupyter Notebook.\\n\\nSteps:\\n1) Read the CSV data containing columns:\\n   CONVERSATION_ID, CONVERSATION_STEP, TEXT, CONTEXT, LABEL\\n2) For each conversation_id, sort by conversation_step, \\n   then build partial -> remainder pairs.\\n3) Tokenize the text (simple approach or real subword approach).\\n4) Train a minimal encoder-decoder Transformer on partial->remainder.\\n5) Demonstrate how to predict the remainder from a partial snippet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conversation Prediction from CSV in a Jupyter Notebook.\n",
    "\n",
    "Steps:\n",
    "1) Read the CSV data containing columns:\n",
    "   CONVERSATION_ID, CONVERSATION_STEP, TEXT, CONTEXT, LABEL\n",
    "2) For each conversation_id, sort by conversation_step, \n",
    "   then build partial -> remainder pairs.\n",
    "3) Tokenize the text (simple approach or real subword approach).\n",
    "4) Train a minimal encoder-decoder Transformer on partial->remainder.\n",
    "5) Demonstrate how to predict the remainder from a partial snippet.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1) Imports\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial->remainder pairs: 76\n",
      "Example pair:\n",
      " {'partial': \"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\\nHi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\\nThe workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process?\", 'remainder': \"You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee?\\nCertainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com.\\nThank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help!\\nYou're welcome, Sam. We look forward to having you at the workshop. Have a great day!\"}\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 2) Read the CSV & Build Partial->Remainder\n",
    "# ========================================\n",
    "\n",
    "\n",
    "def build_convo_pairs_from_csv(csv_path, partial_ratio=0.5):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Group by conversation ID\n",
    "    data_pairs = []\n",
    "    for convo_id, group in df.groupby(\"CONVERSATION_ID\"):\n",
    "        group_sorted = group.sort_values(\"CONVERSATION_STEP\")\n",
    "        # Grab the TEXT column in order\n",
    "        texts = group_sorted[\"TEXT\"].tolist()\n",
    "\n",
    "        if len(texts) < 2:\n",
    "            # If a conversation is too short, skip or handle differently\n",
    "            continue\n",
    "\n",
    "        # partial: first partial_ratio portion\n",
    "        cutoff = max(1, int(len(texts) * partial_ratio))\n",
    "        partial_list = texts[:cutoff]\n",
    "        remainder_list = texts[cutoff:]\n",
    "\n",
    "        partial_str = \"\\n\".join(partial_list)\n",
    "        remainder_str = \"\\n\".join(remainder_list) if remainder_list else \"\"\n",
    "\n",
    "        data_pairs.append({\n",
    "            \"partial\": partial_str.strip(),\n",
    "            \"remainder\": remainder_str.strip()\n",
    "        })\n",
    "    return data_pairs\n",
    "\n",
    "csv_path = \"C:/Users/DELL/Desktop/VOIP_Phishing_Attacks/Repos/convoPredict/conversation-prediction/FINAL_DATASET2.csv\"  # <-- update with your CSV path\n",
    "\n",
    "conversation_data = build_convo_pairs_from_csv(csv_path, partial_ratio=0.5)\n",
    "\n",
    "print(\"Number of partial->remainder pairs:\", len(conversation_data))\n",
    "# Peek at one example\n",
    "if len(conversation_data) > 0:\n",
    "    print(\"Example pair:\\n\", conversation_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2942\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3) Minimal Word-Level Tokenizer\n",
    "# ========================================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.vocab_size = 4\n",
    "\n",
    "    def fit(self, data_pairs):\n",
    "        # We'll collect partial + remainder text\n",
    "        idx = 4\n",
    "        for ex in data_pairs:\n",
    "            combined_text = ex[\"partial\"] + \" \" + ex[\"remainder\"]\n",
    "            words = combined_text.split()\n",
    "            for w in words:\n",
    "                if w not in self.word2idx:\n",
    "                    self.word2idx[w] = idx\n",
    "                    self.idx2word[idx] = w\n",
    "                    idx += 1\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, text, add_bos=False, add_eos=False):\n",
    "        ids = []\n",
    "        if add_bos:\n",
    "            ids.append(self.word2idx[\"<BOS>\"])\n",
    "        for w in text.split():\n",
    "            wid = self.word2idx.get(w, self.word2idx[\"<UNK>\"])\n",
    "            ids.append(wid)\n",
    "        if add_eos:\n",
    "            ids.append(self.word2idx[\"<EOS>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            if i == self.word2idx[\"<EOS>\"]:\n",
    "                break\n",
    "            word = self.idx2word.get(i, \"<UNK>\")\n",
    "            words.append(word)\n",
    "        return \" \".join(words)\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(conversation_data)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4) PyTorch Dataset: partial->remainder\n",
    "# ========================================\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, data_pairs, tokenizer, max_len=50):\n",
    "        self.data = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        partial_enc = self.tokenizer.encode(ex[\"partial\"], add_bos=True, add_eos=True)\n",
    "        remainder_enc = self.tokenizer.encode(ex[\"remainder\"], add_bos=True, add_eos=True)\n",
    "\n",
    "        partial_enc = partial_enc[:self.max_len]\n",
    "        remainder_enc = remainder_enc[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            \"partial_enc\": partial_enc,\n",
    "            \"remainder_enc\": remainder_enc\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    partial_list = [x[\"partial_enc\"] for x in batch]\n",
    "    remainder_list = [x[\"remainder_enc\"] for x in batch]\n",
    "\n",
    "    max_len_partial = max(len(p) for p in partial_list)\n",
    "    max_len_rem = max(len(r) for r in remainder_list)\n",
    "\n",
    "    partial_padded = []\n",
    "    remainder_padded = []\n",
    "    for p, r in zip(partial_list, remainder_list):\n",
    "        p_pad = p + [tokenizer.word2idx[\"<PAD>\"]] * (max_len_partial - len(p))\n",
    "        r_pad = r + [tokenizer.word2idx[\"<PAD>\"]] * (max_len_rem - len(r))\n",
    "        partial_padded.append(p_pad)\n",
    "        remainder_padded.append(r_pad)\n",
    "    \n",
    "    return {\n",
    "        \"partial_batch\": torch.tensor(partial_padded, dtype=torch.long),\n",
    "        \"remainder_batch\": torch.tensor(remainder_padded, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "dataset = ConversationDataset(conversation_data, tokenizer, max_len=50)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_batch shape: torch.Size([2, 50])\n",
      "remainder_batch shape: torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "for batch in loader:\n",
    "    print(\"partial_batch shape:\", batch[\"partial_batch\"].shape)\n",
    "    print(\"remainder_batch shape:\", batch[\"remainder_batch\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5) Minimal Encoder-Decoder Transformer\n",
    "# ========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :].to(x.device)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out_fc = nn.Linear(d_model, vocab_size)\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz))==1).transpose(0,1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # enc_input: [B, seq_len_enc]\n",
    "        # dec_input: [B, seq_len_dec]\n",
    "        # embed\n",
    "        enc_emb = self.embedding(enc_input) * math.sqrt(self.d_model)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "\n",
    "        dec_emb = self.embedding(dec_input) * math.sqrt(self.d_model)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "\n",
    "        # create masks\n",
    "        src_key_padding_mask = (enc_input == self.pad_token_id)\n",
    "        tgt_key_padding_mask = (dec_input == self.pad_token_id)\n",
    "\n",
    "        seq_len_dec = dec_input.size(1)\n",
    "        causal_mask = self._generate_square_subsequent_mask(seq_len_dec).to(enc_input.device)\n",
    "\n",
    "        # pass through transformer\n",
    "        out = self.transformer(\n",
    "            src=enc_emb,\n",
    "            tgt=dec_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        logits = self.out_fc(out)  # [B, seq_len_dec, vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Loss=7.0010\n",
      "Epoch 2/5  Loss=6.2273\n",
      "Epoch 3/5  Loss=5.9828\n",
      "Epoch 4/5  Loss=5.3786\n",
      "Epoch 5/5  Loss=4.6634\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6) Training Loop\n",
    "# ========================================\n",
    "model = TransformerEncoderDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx[\"<PAD>\"])\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        partial_batch = batch[\"partial_batch\"].to(device)\n",
    "        remainder_batch = batch[\"remainder_batch\"].to(device)\n",
    "\n",
    "        # Teacher forcing approach: \n",
    "        # input to decoder = remainder_batch[:, :-1]\n",
    "        # we want to predict remainder_batch[:, 1:]\n",
    "        dec_in = remainder_batch[:, :-1]\n",
    "        labels = remainder_batch[:, 1:].contiguous()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(partial_batch, dec_in)  # [B, dec_len, vocab_size]\n",
    "        B, L, V = logits.shape\n",
    "        loss = criterion(logits.view(B*L, V), labels.view(B*L))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}  Loss={avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTIAL ===\n",
      "Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Org Name] next month. I'm interested in registering but had a few questions.\n",
      "Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography.\n",
      "The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Org Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL'S PREDICTION FOR REMAINDER ===\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7) Inference: Generating the Remainder\n",
    "# ========================================\n",
    "def generate_remainder(model, partial_text, max_gen_len=30):\n",
    "    model.eval()\n",
    "    # encode partial\n",
    "    partial_ids = tokenizer.encode(partial_text, add_bos=True)  # no <EOS> so we can keep appending\n",
    "    src = torch.tensor([partial_ids], dtype=torch.long).to(device)\n",
    "\n",
    "    # Start decoder input with <BOS>\n",
    "    dec_input = [tokenizer.word2idx[\"<BOS>\"]]\n",
    "    dec_tensor = torch.tensor([dec_input], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_gen_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(src, dec_tensor)\n",
    "            # last token in decoder\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "        dec_input.append(next_token_id)\n",
    "        dec_tensor = torch.tensor([dec_input], dtype=torch.long).to(device)\n",
    "\n",
    "        # stop if <EOS>\n",
    "        if next_token_id == tokenizer.word2idx[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    # remove <BOS> from start\n",
    "    generated_ids = dec_input[1:]\n",
    "    # decode until <EOS>\n",
    "    text_out = tokenizer.decode(generated_ids)\n",
    "    return text_out\n",
    "\n",
    "# Let's test on an example partial from the dataset\n",
    "if len(conversation_data) > 0:\n",
    "    test_partial = conversation_data[0][\"partial\"]\n",
    "    print(\"=== PARTIAL ===\")\n",
    "    print(test_partial)\n",
    "    generated = generate_remainder(model, test_partial, max_gen_len=30)\n",
    "    print(\"\\n=== MODEL'S PREDICTION FOR REMAINDER ===\")\n",
    "    print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
