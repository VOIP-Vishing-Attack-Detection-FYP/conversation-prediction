{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_conversations = [\n",
    "    [\n",
    "        \"A: Hello, I’m Sarah from XYZ Bank’s support team. How can I help you today?\",\n",
    "        \"B: Hi Sarah, I was looking to update my address on my account. Could you assist me with that?\",\n",
    "        \"A: Certainly. Could you please verify the last four digits of your account number?\",\n",
    "        \"B: The last four digits are 1234.\",\n",
    "        \"A: Great. I see your current address is 123 Elm Street. What would you like to update it to?\",\n",
    "        \"B: I’d like it changed to 456 Oak Avenue, Springfield.\",\n",
    "        \"A: Perfect. I’ve updated the address. Is there anything else I can help with?\",\n",
    "        \"B: No, that’s all. Thank you so much.\",\n",
    "        \"A: You’re welcome! Have a wonderful day.\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Good morning, this is Max from ABC Internet Services. How may I assist you?\",\n",
    "        \"B: Hi Max, my internet has been running slower than usual. Can you help me figure out why?\",\n",
    "        \"A: Sure, let’s run a quick diagnostic. Could you confirm the email address associated with your account?\",\n",
    "        \"B: It’s jane.doe@example.com.\",\n",
    "        \"A: Thanks. I see there’s some scheduled maintenance in your area which might cause slow speeds. It should be resolved by tomorrow morning.\",\n",
    "        \"B: Got it, thanks for checking. Is there any way to get a temporary speed boost?\",\n",
    "        \"A: Unfortunately, not during maintenance. But I can offer you a small credit for the inconvenience. Would that help?\",\n",
    "        \"B: That would be great. Thanks!\",\n",
    "        \"A: I’ve applied a $5 credit. Anything else I can do?\",\n",
    "        \"B: No, that’s all. Appreciate your help.\",\n",
    "        \"A: My pleasure. Have a nice day!\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Hello, Julie from Secure Payments. How can I help?\",\n",
    "        \"B: Hi Julie, I want to set a travel notice on my credit card.\",\n",
    "        \"A: Absolutely. Could I have the last transaction amount you made so I can verify your identity?\",\n",
    "        \"B: My last transaction was $45 at GroceryMart.\",\n",
    "        \"A: Perfect, I see that. What dates and countries will you be traveling to?\",\n",
    "        \"B: I’ll be in Germany from June 10th to June 20th.\",\n",
    "        \"A: Got it. I’ve placed a travel notice for those dates. You’re all set.\",\n",
    "        \"B: Thank you, that’s all I needed.\",\n",
    "        \"A: You’re welcome. Safe travels!\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vishing_conversations = [\n",
    "    [\n",
    "        \"A: Hello, this is Andrew calling from Premium Bank’s fraud department.\",\n",
    "        \"B: Oh, hi. Is there an issue with my account?\",\n",
    "        \"A: Yes, we noticed several suspicious charges. Could you provide your full account number so we can secure your account immediately?\",\n",
    "        \"B: I’m not comfortable giving my full account number over the phone.\",\n",
    "        \"A: It’s urgent! Your account is at risk right now. If you don’t provide the account and your PIN, we can’t protect your money.\",\n",
    "        \"B: I should call the official bank number before giving this information.\",\n",
    "        \"A: There’s no time. Just give me your PIN, we’ll reverse the charges right now.\",\n",
    "        \"B: I’ll hang up and check with the bank directly. Goodbye.\",\n",
    "        \"A: Wait, no, don’t disconnect—!\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Hi, I’m calling from the government tax office. We have an urgent notice for you.\",\n",
    "        \"B: The tax office? Is there a problem?\",\n",
    "        \"A: Yes, there is a warrant for your arrest due to unpaid taxes. To fix this, you need to pay immediately.\",\n",
    "        \"B: That sounds suspicious. I don’t think the tax office calls like this.\",\n",
    "        \"A: If you don’t give me your credit card number right now, the police will be at your door in an hour.\",\n",
    "        \"B: No, I’m going to hang up and verify this through official channels.\",\n",
    "        \"A: Don’t you dare hang up! You must pay now!\",\n",
    "        \"B: (Hangs up)\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Good afternoon, this is Alex from Techy Support for your mobile service.\",\n",
    "        \"B: Hi, what’s the issue?\",\n",
    "        \"A: Your phone has been compromised. To fix it, I need your password and PIN so I can access your device remotely.\",\n",
    "        \"B: That’s not normal procedure.\",\n",
    "        \"A: It’s an emergency! Hackers are stealing your data. Give me your PIN so I can lock them out.\",\n",
    "        \"B: I’m going to call the official support line and verify.\",\n",
    "        \"A: No time! They’ll steal everything right now if you don’t comply!\",\n",
    "        \"B: I don’t believe you. Goodbye.\",\n",
    "        \"A: Wait, I…!\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversations = legal_conversations + vishing_conversations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pre trained vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\envs\\ai-backend\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the pre-trained BERT model to extract embeddings\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Freeze BERT parameters if you don't want to fine-tune them\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Only CPU is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, conversations, tokenizer, max_length=512):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for conv in conversations:\n",
    "            # Flatten the conversation into a single string\n",
    "            conv_text = \" \".join(conv)\n",
    "            \n",
    "            # Tokenize and encode\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                conv_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].squeeze(0)  # Shape: [max_length]\n",
    "            attention_mask = encoding['attention_mask'].squeeze(0)  # Shape: [max_length]\n",
    "            \n",
    "            # For language modeling, target is the input shifted by one\n",
    "            # So, input is tokens 0 to n-2, target is tokens 1 to n-1\n",
    "            self.inputs.append(input_ids[:-1])\n",
    "            self.targets.append(input_ids[1:])\n",
    "            self.attention_masks.append(attention_mask[:-1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.attention_masks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inp_batch = [item[0] for item in batch]\n",
    "    tgt_batch = [item[1] for item in batch]\n",
    "    mask_batch = [item[2] for item in batch]\n",
    "\n",
    "    inp_padded = torch.stack(inp_batch)\n",
    "    tgt_padded = torch.stack(tgt_batch)\n",
    "    mask_padded = torch.stack(mask_batch).unsqueeze(1).unsqueeze(2)  # [B, 1, 1, S]\n",
    "    \n",
    "    return inp_padded, tgt_padded, mask_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = ConversationDataset(all_conversations, tokenizer, max_length=128)  # Adjust max_length as needed\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create constant 'pe' matrix with values dependent on\n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = None  # To store attention weights\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Perform linear operation and split into h heads\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)  # [batch_size, num_heads, seq_len, d_k]\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        self.attention = attn\n",
    "        \n",
    "        out = torch.matmul(attn, v)  # [batch_size, num_heads, seq_len, d_k]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Final linear layer\n",
    "        out = self.out(out)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.mha(x, x, x, mask)  # Self-attention\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feedforward\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, vocab_size, max_len=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        out = self.fc_out(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Transformer Model with Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BERT's embedding weights\n",
    "pretrained_embeddings = bert_model.embeddings.word_embeddings.weight.data  # [vocab_size, hidden_size]\n",
    "\n",
    "# Define model parameters\n",
    "d_model = pretrained_embeddings.size(1)  # Typically 768 for BERT-base\n",
    "num_heads = 12  # For BERT-base\n",
    "num_layers = 6  # Number of Transformer encoder layers\n",
    "d_ff = 3072  # Typically 4 * d_model\n",
    "dropout = 0.1\n",
    "vocab_size = tokenizer.vocab_size  # Size of BERT's tokenizer vocabulary\n",
    "max_len = 128  # Adjust based on your dataset\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = TransformerModel(d_model, num_heads, num_layers, d_ff, vocab_size, max_len, dropout)\n",
    "\n",
    "# Initialize embedding layer with pre-trained embeddings\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Optionally, freeze embedding layer to prevent updating during training\n",
    "model.embedding.weight.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 9.7168\n",
      "Epoch 2/5, Loss: 8.4117\n",
      "Epoch 3/5, Loss: 8.0370\n",
      "Epoch 4/5, Loss: 7.7322\n",
      "Epoch 5/5, Loss: 7.4145\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inp, tgt, mask = batch\n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inp, mask)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Reshape outputs and targets for loss computation\n",
    "        outputs = outputs.view(-1, vocab_size)  # [batch_size * seq_len, vocab_size]\n",
    "        tgt = tgt.view(-1)  # [batch_size * seq_len]\n",
    "        \n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation(prefix_lines, tokenizer, model, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Flatten the prefix lines into a single string\n",
    "        prefix_text = \" \".join(prefix_lines)\n",
    "        \n",
    "        # Tokenize and encode\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            prefix_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(device)  # [1, S]\n",
    "        attention_mask = encoding['attention_mask'].to(device)  # [1, S]\n",
    "        \n",
    "        # Initialize generated sequence with input_ids\n",
    "        generated = input_ids\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Generate mask\n",
    "            mask = (generated != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)  # [1, 1, 1, S]\n",
    "            \n",
    "            # Get the model's predictions\n",
    "            outputs = model(generated, mask)  # [1, S, vocab_size]\n",
    "            logits = outputs[:, -1, :]  # [1, vocab_size]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Greedy decoding: select the token with highest probability\n",
    "            next_token = torch.argmax(probs, dim=-1).unsqueeze(0)  # [1,1]\n",
    "            \n",
    "            # Append the predicted token to the generated sequence\n",
    "            generated = torch.cat((generated, next_token), dim=1)  # [1, S+1]\n",
    "            \n",
    "            # If EOS token is generated, stop\n",
    "            if next_token.item() == tokenizer.sep_token_id or next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        # Decode the generated tokens to text\n",
    "        generated_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "        \n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given prefix:\n",
      "A: Hello.\n",
      "B: Hello, yes\n",
      "\n",
      "Predicted continuation:\n",
      "a : hello. b : hello, yes..................................................\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "partial_conversation = [\n",
    "    \"A: Hello.\",\n",
    "    \"B: Hello, yes\"\n",
    "]\n",
    "\n",
    "continuation = generate_continuation(partial_conversation, tokenizer, model, max_length=50)\n",
    "\n",
    "print(\"\\nGiven prefix:\")\n",
    "for line in partial_conversation:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\nPredicted continuation:\")\n",
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
