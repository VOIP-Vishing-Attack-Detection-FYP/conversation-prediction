{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1. Sample Conversation Data\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_conversations = [\n",
    "    [\n",
    "        \"A: Hello, I’m Sarah from XYZ Bank’s support team. How can I help you today?\",\n",
    "        \"B: Hi Sarah, I was looking to update my address on my account. Could you assist me with that?\",\n",
    "        \"A: Certainly. Could you please verify the last four digits of your account number?\",\n",
    "        \"B: The last four digits are 1234.\",\n",
    "        \"A: Great. I see your current address is 123 Elm Street. What would you like to update it to?\",\n",
    "        \"B: I’d like it changed to 456 Oak Avenue, Springfield.\",\n",
    "        \"A: Perfect. I’ve updated the address. Is there anything else I can help with?\",\n",
    "        \"B: No, that’s all. Thank you so much.\",\n",
    "        \"A: You’re welcome! Have a wonderful day.\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Good morning, this is Max from ABC Internet Services. How may I assist you?\",\n",
    "        \"B: Hi Max, my internet has been running slower than usual. Can you help me figure out why?\",\n",
    "        \"A: Sure, let’s run a quick diagnostic. Could you confirm the email address associated with your account?\",\n",
    "        \"B: It’s jane.doe@example.com.\",\n",
    "        \"A: Thanks. I see there’s some scheduled maintenance in your area which might cause slow speeds.\",\n",
    "        \"B: Got it, thanks for checking. Is there any way to get a temporary speed boost?\",\n",
    "        \"A: Unfortunately, not during maintenance. But I can offer you a small credit for the inconvenience.\",\n",
    "        \"B: That would be great. Thanks!\",\n",
    "        \"A: I’ve applied a $5 credit. Anything else I can do?\",\n",
    "        \"B: No, that’s all. Appreciate your help.\",\n",
    "        \"A: My pleasure. Have a nice day!\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "vishing_conversations = [\n",
    "    [\n",
    "        \"A: Hello, this is Andrew calling from Premium Bank’s fraud department.\",\n",
    "        \"B: Oh, hi. Is there an issue with my account?\",\n",
    "        \"A: Yes, we noticed several suspicious charges. Could you provide your full account number?\",\n",
    "        \"B: I’m not comfortable giving my full account number over the phone.\",\n",
    "        \"A: It’s urgent! If you don’t provide the account and your PIN, we can’t protect your money.\",\n",
    "        \"B: I should call the official bank number before giving this information.\",\n",
    "        \"A: There’s no time. Just give me your PIN now!\",\n",
    "        \"B: I’ll hang up and check with the bank directly. Goodbye.\",\n",
    "        \"A: Wait, no, don’t disconnect—!\"\n",
    "    ],\n",
    "    [\n",
    "        \"A: Hi, I’m calling from the government tax office. We have an urgent notice for you.\",\n",
    "        \"B: The tax office? Is there a problem?\",\n",
    "        \"A: Yes, there is a warrant for your arrest due to unpaid taxes. You need to pay immediately.\",\n",
    "        \"B: That sounds suspicious. I don’t think the tax office calls like this.\",\n",
    "        \"A: If you don’t give me your credit card number, the police will be at your door soon.\",\n",
    "        \"B: No, I’m going to hang up and verify through official channels.\",\n",
    "        \"A: Don’t hang up! You must pay now!\",\n",
    "        \"B: (Hangs up)\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "all_conversations = legal_conversations + vishing_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2. Build a Vocabulary\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN = SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "        self.pad_index = None\n",
    "        self.bos_index = None\n",
    "        self.eos_index = None\n",
    "        self.unk_index = None\n",
    "    \n",
    "    def build_vocab(self, text_list, min_freq=1):\n",
    "        \"\"\"\n",
    "        text_list: A list of strings from which to build a vocabulary.\n",
    "        min_freq : Minimum frequency for a token to be included (optional).\n",
    "        \"\"\"\n",
    "        # Step 1: Collect frequency\n",
    "        freq = {}\n",
    "        for text in text_list:\n",
    "            for token in text.split():\n",
    "                freq[token] = freq.get(token, 0) + 1\n",
    "        \n",
    "        # Step 2: Initialize with special tokens\n",
    "        idx = 0\n",
    "        for st in SPECIAL_TOKENS:\n",
    "            self.token2idx[st] = idx\n",
    "            self.idx2token.append(st)\n",
    "            idx += 1\n",
    "        \n",
    "        # Step 3: Add tokens based on frequency\n",
    "        for token, count in freq.items():\n",
    "            if count >= min_freq and token not in self.token2idx:\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "                idx += 1\n",
    "        \n",
    "        # Step 4: Store indices for quick access\n",
    "        self.pad_index = self.token2idx[PAD_TOKEN]\n",
    "        self.bos_index = self.token2idx[BOS_TOKEN]\n",
    "        self.eos_index = self.token2idx[EOS_TOKEN]\n",
    "        self.unk_index = self.token2idx[UNK_TOKEN]\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Simple whitespace split; in real use-cases consider advanced tokenization\n",
    "        return text.split()\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        # Convert text to list of token indices\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.token2idx.get(t, self.unk_index) for t in tokens]\n",
    "    \n",
    "    def denumericalize(self, indices):\n",
    "        # Convert list of token indices back to text\n",
    "        return [self.idx2token[idx] for idx in indices]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 260\n"
     ]
    }
   ],
   "source": [
    "# Flatten all lines in the dataset to build vocabulary\n",
    "all_lines = []\n",
    "for conv in all_conversations:\n",
    "    for line in conv:\n",
    "        # We'll just keep raw text. \n",
    "        # For advanced usage, remove punctuation, lower-case, etc.\n",
    "        line_clean = line.strip()\n",
    "        all_lines.append(line_clean)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(all_lines, min_freq=1)\n",
    "vocab_size = vocab.vocab_size\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3. Preparing the Dataset\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For each conversation, we flatten the lines into one \n",
    "    sequence:  <BOS> line1 <EOS> line2 <EOS> line3 <EOS> ...\n",
    "    The next-token prediction objective:\n",
    "      - Input: [<BOS>, line1 tokens, <EOS>, line2 tokens, ...]\n",
    "      - Target: same sequence shifted 1 to the right\n",
    "    \"\"\"\n",
    "    def __init__(self, conversations, vocab, max_length=128):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        for conv in conversations:\n",
    "            # Flatten conversation lines with <EOS> in between\n",
    "            # e.g. \"<BOS> A: Hello ... <EOS> B: Hi ... <EOS> ...\"\n",
    "            token_list = [vocab.bos_index]  # start with <BOS>\n",
    "            \n",
    "            for line in conv:\n",
    "                line_tokens = vocab.numericalize(line)\n",
    "                # Add line tokens + <EOS>\n",
    "                token_list.extend(line_tokens)\n",
    "                token_list.append(vocab.eos_index)\n",
    "            \n",
    "            # If conversation too long, truncate\n",
    "            if len(token_list) > self.max_length:\n",
    "                token_list = token_list[:self.max_length]\n",
    "            \n",
    "            self.samples.append(token_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of token sequences of different lengths.\n",
    "    We pad them to the max length in the batch.\n",
    "    Then create input/target by shifting by 1 for next-token prediction.\n",
    "    \"\"\"\n",
    "    # Find the longest sequence in the batch\n",
    "    max_len = max(len(seq) for seq in batch)\n",
    "    \n",
    "    padded_inp = []\n",
    "    padded_tgt = []\n",
    "    \n",
    "    for seq in batch:\n",
    "        # Input is seq[:-1], target is seq[1:]\n",
    "        # But we first pad to max_len\n",
    "        inp_seq = seq[:-1]  # all but last\n",
    "        tgt_seq = seq[1:]   # all but first\n",
    "        \n",
    "        # Pad input and target to max_len-1\n",
    "        if len(inp_seq) < max_len-1:\n",
    "            inp_seq += [vocab.pad_index] * (max_len - 1 - len(inp_seq))\n",
    "        if len(tgt_seq) < max_len-1:\n",
    "            tgt_seq += [vocab.pad_index] * (max_len - 1 - len(tgt_seq))\n",
    "        \n",
    "        padded_inp.append(inp_seq)\n",
    "        padded_tgt.append(tgt_seq)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    inp_tensor = torch.tensor(padded_inp, dtype=torch.long)\n",
    "    tgt_tensor = torch.tensor(padded_tgt, dtype=torch.long)\n",
    "    return inp_tensor, tgt_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = ConversationDataset(all_conversations, vocab, max_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 4. Decoder-Only Transformer (from scratch)\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        We add positional encoding to x.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # x + pe[:, :seq_len, :]\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads.\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        mask: [batch_size, seq_len, seq_len] or broadcastable shape\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1) Linear projection\n",
    "        q = self.q_linear(x)  # [bsz, seq_len, d_model]\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        \n",
    "        # 2) Split into heads\n",
    "        q = q.view(bsz, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # [bsz, n_heads, seq_len, d_k]\n",
    "        k = k.view(bsz, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(bsz, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3) Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # [bsz, n_heads, seq_len, seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)  # [bsz, n_heads, seq_len, seq_len]\n",
    "        out = torch.matmul(attn, v)  # [bsz, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # 4) Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(bsz, seq_len, self.d_model)  # [bsz, seq_len, d_model]\n",
    "        \n",
    "        # 5) Final linear layer\n",
    "        out = self.out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.layernorm1(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.layernorm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 d_model=256,\n",
    "                 n_heads=4,\n",
    "                 num_layers=3,\n",
    "                 d_ff=1024,\n",
    "                 max_len=512,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def generate_causal_mask(self, seq_len, device):\n",
    "        \"\"\"\n",
    "        Generate an upper-triangular causal mask \n",
    "        so each token can only attend to tokens on its left.\n",
    "        Shape: [seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones((seq_len, seq_len), device=device), diagonal=1)\n",
    "        # 1 = no access (future), 0 = can attend\n",
    "        mask = (mask == 0)  # invert\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len]\n",
    "        Returns: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        bsz, seq_len = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.token_emb(x)  # [bsz, seq_len, d_model]\n",
    "        x = self.pos_emb(x)    # add positional encodings\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = self.generate_causal_mask(seq_len, device)  # [seq_len, seq_len]\n",
    "        # We need a broadcastable shape [bsz, n_heads, seq_len, seq_len], \n",
    "        # but let's just keep it [seq_len, seq_len] if we handle it in attention.\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        \n",
    "        logits = self.fc_out(x)  # [bsz, seq_len, vocab_size]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 5. Training Loop\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_layers = 3\n",
    "d_ff = 1024\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOnlyTransformer(\n",
       "  (token_emb): Embedding(260, 256)\n",
       "  (pos_emb): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x DecoderLayer(\n",
       "      (self_attn): MultiHeadSelfAttention(\n",
       "        (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): FeedForward(\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (layernorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=260, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_len=128,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_index)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.5071\n",
      "Epoch 2/5, Loss: 4.6805\n",
      "Epoch 3/5, Loss: 4.0514\n",
      "Epoch 4/5, Loss: 3.3875\n",
      "Epoch 5/5, Loss: 2.8585\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inp, tgt in dataloader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inp)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Flatten for loss calculation: (batch * seq_len, vocab_size)\n",
    "        # And compare with target: (batch * seq_len)\n",
    "        logits_2d = logits.view(-1, vocab_size)\n",
    "        tgt_1d = tgt.view(-1)\n",
    "        \n",
    "        loss = criterion(logits_2d, tgt_1d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 6. Generation (Autoregressive Inference)\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token indices: [1, 4, 5, 15, 3, 3, 8, 43, 3, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15, 14, 15]\n",
      "\n",
      "Generated text (tokens):\n",
      "['<BOS>', 'A:', 'Hello,', 'I', '<UNK>', '<UNK>', 'from', 'your', '<UNK>', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I', 'can', 'I']\n",
      "\n",
      "Final Output Text:\n",
      "<BOS> A: Hello, I <UNK> <UNK> from your <UNK> can I can I can I can I can I can I can I can I can I can I can I can I can I can I can I\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prefix, model, vocab, max_new_tokens=20):\n",
    "    \"\"\"\n",
    "    prefix: list of tokens (indices) that serve as the context.\n",
    "    model: the trained model.\n",
    "    Return the entire sequence (prefix + newly generated tokens).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(prefix, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        # shape: [1, prefix_len]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            logits = model(x)  # [1, current_len, vocab_size]\n",
    "            \n",
    "            # Get last token's logits\n",
    "            last_token_logits = logits[:, -1, :]  # [1, vocab_size]\n",
    "            \n",
    "            # Greedy\n",
    "            next_token = torch.argmax(last_token_logits, dim=-1)  # [1]\n",
    "            # next_token = sample_from_probs(last_token_logits)  # for sampling\n",
    "            \n",
    "            if next_token.item() == vocab.eos_index:\n",
    "                # Stop if we hit <EOS>\n",
    "                break\n",
    "            \n",
    "            # Append to sequence\n",
    "            x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        return x.squeeze(0).tolist()  # [full_sequence_length]\n",
    "\n",
    "# Example usage: Provide a partial line as prefix\n",
    "prefix_text = \"A: Hello, I am John from your bank.\"\n",
    "prefix_tokens = [vocab.bos_index] + vocab.numericalize(prefix_text)\n",
    "\n",
    "generated_indices = generate_text(prefix_tokens, model, vocab, max_new_tokens=30)\n",
    "generated_text = vocab.denumericalize(generated_indices)\n",
    "\n",
    "print(\"Generated token indices:\", generated_indices)\n",
    "print(\"\\nGenerated text (tokens):\")\n",
    "print(generated_text)\n",
    "\n",
    "# Convert to string (roughly)\n",
    "output_string = \" \".join(generated_text)\n",
    "print(\"\\nFinal Output Text:\")\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_conversation = [\n",
    "    \"A: Hello, I am John from your bank.\",\n",
    "    \"B: Hi John, I was expecting your call.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token indices: [1, 4, 5, 15, 3, 3, 8, 43, 3, 19, 20, 3, 15, 22, 3, 43, 3, 17, 31]\n",
      "\n",
      "Generated text (tokens):\n",
      "['<BOS>', 'A:', 'Hello,', 'I', '<UNK>', '<UNK>', 'from', 'your', '<UNK>', 'B:', 'Hi', '<UNK>', 'I', 'was', '<UNK>', 'your', '<UNK>', 'you', 'assist']\n",
      "\n",
      "Final Output Text:\n",
      "<BOS> A: Hello, I <UNK> <UNK> from your <UNK> B: Hi <UNK> I was <UNK> your <UNK> you assist\n"
     ]
    }
   ],
   "source": [
    "# 1. Combine the partial lines into a single string\n",
    "prefix_text = \" \".join(partial_conversation)\n",
    "\n",
    "# 2. Convert text to token indices\n",
    "prefix_tokens = [vocab.bos_index] + vocab.numericalize(prefix_text)\n",
    "\n",
    "# 3. Generate text\n",
    "generated_indices = generate_text(prefix_tokens, model, vocab, max_new_tokens=50)\n",
    "generated_text = vocab.denumericalize(generated_indices)\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"Generated token indices:\", generated_indices)\n",
    "print(\"\\nGenerated text (tokens):\")\n",
    "print(generated_text)\n",
    "\n",
    "# 5. Optionally join tokens into a single string for readability\n",
    "output_string = \" \".join(generated_text)\n",
    "print(\"\\nFinal Output Text:\")\n",
    "print(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
