{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "# 1. Creating a Synthetic Dataset\n",
    "#\n",
    "# For illustration, let's say we have some labeled conversations.\n",
    "# Each conversation has a series of utterances, and at the end\n",
    "# we have a label: 0 = legitimate, 1 = vishing attack.\n",
    "#\n",
    "# We also want the model to predict the next part of the conversation\n",
    "# based on what has been said so far.\n",
    "#\n",
    "# In a real scenario, you'd have real conversations and ground truth.\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "    ([\"[CLS]\", \"hello\", \"this\", \"is\", \"bank\", \"security\", \"team\"],      # Input so far\n",
    "     [\"we\", \"need\", \"your\", \"account\", \"details\"],                      # Next to predict\n",
    "     1), # vishing (fake scenario)\n",
    "    ([\"[CLS]\", \"hi\", \"i\", \"am\", \"calling\", \"about\", \"your\", \"internet\"], \n",
    "     [\"please\", \"confirm\", \"your\", \"password\", \"now\"], \n",
    "     1), # vishing\n",
    "    ([\"[CLS]\", \"hello\", \"how\", \"can\", \"i\", \"help\", \"you\", \"today\"], \n",
    "     [\"i\", \"am\", \"just\", \"checking\", \"my\", \"balance\"], \n",
    "     0), # legitimate\n",
    "    ([\"[CLS]\", \"hi\", \"this\", \"is\", \"abc\", \"bank\"], \n",
    "     [\"we\", \"just\", \"need\", \"to\", \"confirm\", \"your\", \"last\", \"transaction\"], \n",
    "     0)  # legitimate\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a small vocabulary from these tokens.\n",
    "all_tokens = []\n",
    "for conv_inp, conv_out, label in conversations:\n",
    "    all_tokens.extend(conv_inp)\n",
    "    all_tokens.extend(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "CLS_TOKEN = \"[CLS]\"  # already in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build vocab\n",
    "vocab = {PAD_TOKEN:0, UNK_TOKEN:1, BOS_TOKEN:2, EOS_TOKEN:3, CLS_TOKEN:4}\n",
    "for tok in all_tokens:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "inv_vocab = {v:k for k,v in vocab.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentence(sentence, vocab, bos=True, eos=True):\n",
    "    tokens = []\n",
    "    # We'll only add BOS/EOS for the parts we want to predict (the target)\n",
    "    # For input (which already has [CLS]), we might skip BOS since [CLS] can serve as a start.\n",
    "    # But here we'll just be consistent and add BOS/EOS to targets. \n",
    "    # Inputs already have [CLS] so we might skip BOS for them.\n",
    "    # Let's say we don't add BOS/EOS to input, just to targets for language modeling.\n",
    "    encoded = []\n",
    "    for w in sentence:\n",
    "        encoded.append(vocab[w] if w in vocab else vocab[UNK_TOKEN])\n",
    "    tokens = torch.tensor(encoded, dtype=torch.long)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(sentence, vocab):\n",
    "    # For the target, we add BOS and EOS to define the predicted sequence boundaries.\n",
    "    encoded = [vocab[BOS_TOKEN]]\n",
    "    for w in sentence:\n",
    "        encoded.append(vocab[w] if w in vocab else vocab[UNK_TOKEN])\n",
    "    encoded.append(vocab[EOS_TOKEN])\n",
    "    return torch.tensor(encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# 2. Dataset & Dataloader\n",
    "#\n",
    "# Each item: (input_ids, target_ids, label)\n",
    "# We'll have the model predict the target tokens and classify.\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VishingDataset(Dataset):\n",
    "    def __init__(self, conversations, vocab):\n",
    "        self.data = conversations\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt, lbl = self.data[idx]\n",
    "        inp_ids = encode_sentence(inp, self.vocab, bos=False, eos=False) \n",
    "        tgt_ids = encode_target(tgt, self.vocab)  # includes BOS/EOS\n",
    "        label = torch.tensor(lbl, dtype=torch.float)\n",
    "        return inp_ids, tgt_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch: list of (inp_ids, tgt_ids, label)\n",
    "    inp_batch = [b[0] for b in batch]\n",
    "    tgt_batch = [b[1] for b in batch]\n",
    "    labels = torch.stack([b[2] for b in batch], dim=0)\n",
    "\n",
    "    inp_padded = pad_sequence(inp_batch, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
    "\n",
    "    # src_mask for Transformer encoder (mask out PAD)\n",
    "    # shape: (B, 1, 1, S)\n",
    "    src_mask = (inp_padded != vocab[PAD_TOKEN]).unsqueeze(1).unsqueeze(2)\n",
    "    return inp_padded, tgt_padded, labels, src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VishingDataset(conversations, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ab1b7189d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# 3. Model Implementation\n",
    "#\n",
    "# We will add a classification head on top of the encoder output.\n",
    "# The classification will be done based on the hidden state of [CLS].\n",
    "#\n",
    "# We also predict the next sequence tokens.\n",
    "# So the loss is a combination of:\n",
    "# - Language Modeling loss (predicting tgt_ids from encoder output)\n",
    "# - Classification loss (binary classification from [CLS] state)\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0) # shape (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attention_weights, value), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        def transform(x, linear_layer):\n",
    "            x = linear_layer(x)\n",
    "            return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        query = transform(query, self.linear_q)\n",
    "        key   = transform(key,   self.linear_k)\n",
    "        value = transform(value, self.linear_v)\n",
    "\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        # Concatenate attention heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        return self.fc_out(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedforward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # Self-attention\n",
    "        src2 = self.attention(src, src, src, src_mask)\n",
    "        src = self.layer_norm1(src + self.dropout(src2))\n",
    "        \n",
    "        # Feed-forward\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = self.layer_norm2(src + self.dropout(src2))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVishingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout):\n",
    "        super(TransformerVishingModel, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout)\n",
    "        # For language modeling (next conversation tokens)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        # For classification (vishing or not)\n",
    "        self.cls_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src: (B, S)\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        # enc_output: (B, S, d_model)\n",
    "        # The classification token [CLS] is at src[:,0], so let's use that:\n",
    "        cls_emb = enc_output[:, 0, :]  # (B, d_model)\n",
    "        # Classification output:\n",
    "        cls_output = self.cls_head(cls_emb)  # (B, 1)\n",
    "\n",
    "        # Language modeling output: predict next tokens for the entire sequence.\n",
    "        # Usually, you'd train a decoder for this, but here we do a simple LM over the encoder output.\n",
    "        # This means we are using a causal scenario or we might just treat this as a reconstruction.\n",
    "        # In a real scenario, you'd likely have a separate decoder for prediction of next tokens.\n",
    "        # For simplicity, we'll just produce output for each token position.\n",
    "        lm_output = self.fc_out(enc_output)  # (B, S, vocab_size)\n",
    "\n",
    "        return lm_output, cls_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "# 4. Training Loop\n",
    "#\n",
    "# We will train using a joint loss:\n",
    "# - LM Loss: CrossEntropy on predicted tokens vs. target tokens\n",
    "# - Classification Loss: Binary cross entropy on cls_output vs. label\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "d_model = 128\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerVishingModel(vocab_size, d_model, num_layers, num_heads, d_ff, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "lm_criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
    "cls_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.2834\n",
      "Epoch 2, Loss: 3.3252\n",
      "Epoch 3, Loss: 2.7470\n",
      "Epoch 4, Loss: 2.0891\n",
      "Epoch 5, Loss: 1.5366\n",
      "Epoch 6, Loss: 1.2184\n",
      "Epoch 7, Loss: 0.9861\n",
      "Epoch 8, Loss: 0.7602\n",
      "Epoch 9, Loss: 0.5577\n",
      "Epoch 10, Loss: 0.4800\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inp_padded, tgt_padded, labels, src_mask in dataloader:\n",
    "        inp_padded = inp_padded.to(device)\n",
    "        tgt_padded = tgt_padded.to(device)\n",
    "        labels = labels.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lm_output, cls_output = model(inp_padded, src_mask) \n",
    "        # lm_output: (B, S_in, vocab_size)\n",
    "\n",
    "        # Shift targets by removing the BOS token (assuming tgt_padded includes BOS)\n",
    "        # If tgt_padded = [BOS, w1, w2, w3, EOS], after dropping BOS:\n",
    "        # tgt_padded = [w1, w2, w3, EOS]\n",
    "        # This should align with the model's predictions for [CLS, w1, w2, w3].\n",
    "        tgt_padded = tgt_padded[:, 1:]  # Remove BOS\n",
    "\n",
    "        # Now ensure both sequences are the same length\n",
    "        min_length = min(lm_output.size(1), tgt_padded.size(1))\n",
    "        lm_output = lm_output[:, :min_length, :]\n",
    "        tgt_padded = tgt_padded[:, :min_length]\n",
    "\n",
    "        # Compute language modeling loss\n",
    "        lm_loss = lm_criterion(lm_output.reshape(-1, vocab_size), tgt_padded.reshape(-1))\n",
    "\n",
    "        # Classification loss\n",
    "        cls_loss = cls_criterion(cls_output.view(-1), labels)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = lm_loss + cls_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# 5. Inference\n",
    "#\n",
    "# Example: given a partial conversation, we predict next tokens and classify.\n",
    "###############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['[CLS]', 'hello', 'this', 'is', 'bank', 'security', 'team']\n",
      "True Label (vishing=1): 1.0\n",
      "Predicted Label: 0.9770664572715759 (>0.5 means vishing)\n",
      "Predicted Next Tokens: ['we', 'need', 'your', 'account', 'details', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Take first example from dataset\n",
    "    inp, tgt, lbl = dataset[0]\n",
    "    # inp: [CLS, hello, this, is, bank, security, team]\n",
    "    # tgt: next utterance\n",
    "    inp = inp.unsqueeze(0).to(device)\n",
    "    src_mask = (inp != vocab[PAD_TOKEN]).unsqueeze(1).unsqueeze(2).to(device)\n",
    "    lm_output, cls_output = model(inp, src_mask)\n",
    "    pred_tokens = lm_output.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
    "    pred_label = torch.sigmoid(cls_output).item()\n",
    "    print(\"Input:\", [inv_vocab[i.item()] for i in dataset[0][0]])\n",
    "    print(\"True Label (vishing=1):\", lbl.item())\n",
    "    print(\"Predicted Label:\", pred_label, \"(>0.5 means vishing)\")\n",
    "\n",
    "    predicted_words = [inv_vocab[t] for t in pred_tokens]\n",
    "    print(\"Predicted Next Tokens:\", predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conversation-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
